#! Multiple Dispatch and multi argument procedures

later versions of this programming language are going
to have multi dispatch, by this mechanism binary
operations are going to be built. until i can figure out
mutiple arguments (and i kind of want a working
version sooner than that would allow) single arguments
are the implementation. this gives rise to a problem,
namely, how do we have binops at all?
well, i guess, to start there will only be primitive
operations, and those are going to be handled in a
different way than composite binops.
(boom, it's done, we even encapsulate the precise
primitiveness/compositeness from the interpreter
via the command pattern. i.e. a function pointer.)

(and even later the ability to
pass a list of arguments, i.e. varargs).

#! optimizations

our interpreter has the ability to
apply monotyped procedures exclusively.
how is this you ask? doesn't this language
have polymorphic procedures? well,
since the types of the actual arguments are knowable,
 we can uniquely dispatch to one of
the instances of that polymorphic procedure, and given
that the types are monomorphic, by definition, this makes
the application site reducable/optimizable to a direct call to the
instance of the polymorphic procedure, right?

(even if this procedure were to call a polymorphic
procedure, this procedure is only able to pass
monotyped arguments into that polymorphic procedure,
by definition)

sure we could apply more than one call, but each application
site is itself monomorphic. this just repeats the above
argument, but for some given set of monomorphic types.

in the reverse case, a polymorphic procedure
calling another polymorphic procedure implies that both
polymorphic procedures are valid given the exact same
set of types. to call one passing in some monomorph,
by definition, implies that the other polymorphic procedure
must be valid for that same monotype, (or not if the programmer
doesn't pass the polytype argument into the poly procedure.)

it is only in the case of a polymorphic argument being passed
in as a variable to substitute for that we would see that the
two polymorphs are now tied in the monomorphic types they
must be valid for. (every type that makes the one typecheck
must make the other typecheck or else typing of any application
of the one containing the other would fail.)


to repeat:
a monotyped procedure applies another monotyped procedure
within it's body, this results in a monotyped application
site.
extending this into the four cases i can think of:
mono -> mono => mono
mono -> poly => mono
poly -> mono => mono
poly -> poly => poly

if you were to write procedures which only had polymorphic type,
and you only ever called procedures with polymorphic type.
(by definition never introducing a monotyped anything)
you would be saying a lot of nothing.
until such time as you apply the procedures giving some
concrete type, then, the body must type given that the
type provided is what the variables type is.


#! use before definition

// technically during parsing, typeing, and evaluation
// of a procedures body,
// the names of the argument list are available for use
// within the body of a procedure.
// however we (eventually) want to parse terms which may use
// names before they have a definition. either the name is
// recursive, or mutually recursive with another name.
// recursion and especially mutual recursion are useful.
// this means we do not want to introduce a dependency
// on names within the parser. so that it is easier
// to provide usage before definition, which makes
// the programmers job easier with respect to
// forward declarations. and module inclusion in
// general. we simply union the sets of names which
// are explicitly exported by each module in the program
// to be the complete set of globally available names.
// and until such time as we have fully parsed every module
// we cannot assume to have the definition of each of these
// names. therefore, when we parse one of the modules from
// the set of known modules, we can construct a set of terms/
// module-definitions which are incomplete due to the lack
// of a definition for one of the names used within the term.
// then in order to complete the parsing of the module we can retry
// each of the terms in each of the modules incomplete definition
// set, to hopefully define some of the names. this can continue
// until we have parsed every available module.
// at such time we need to check if any of the modules still have
// incomplete definitions, and then only after fully collecting
// all of the source text can we for sure say that the undefined
// names are actually fully not present within the source text.
if you imagine each set of exported names from each module,
firstly, we don't allow redefinition. secondly each name is known
before it's term is when we read the header of said module and
declare the set of names to be exported. we could imagine a procedure
which operates over a multimap of names to terms, which names are
mapped to the same term? well, how about each of the names within
the term which were undefined/not-lookup-able at time of typechecking?
then later when we encounter a names definition we can use the multimap
perhaps to retreive the undefined term, given that we just defined a new
name, perhaps the old term is now typeable under the new context within
which the name has a definition, and thus a type, and thus operations
continue normally from that point.


------------------------------------------------------------------
functions and variables work together
over the symbol-table data structure
to provide abstraction to the language.

operations and values work together
over the operator-table data structure
to provide expressions to the language.

binops are operations over two values
unops  are operations over one value

types connect these two constructs,
abstraction and expression, and allow
programmers to combine the two nearly
indiscriminately.

types, therefore are what mainly manages
the composition of language constructs.

also, funnily enough, primitive values
plus operations also allow you to describe
types as a kind of affix expression.

type := Nil
      | Int
      | Bool
      | type '->' type
      | '(' type ')'

and i can imagine
      | type '+' type
      | type '|' type

these of course all follow the
same grammatical pattern
[type binary-operator type]
which means that type constructors
can be considered primitive language
operations. which tracks.
this also allows for a direct meaning
of typenames, they can work exactly the
same as variables, allowing for a type
term/tree to be saved and used to construct
further types. recursive types will still
need to be handled by some language mechanism.
perhaps, out of order declarations? we allow
some leeway in using a name before we can find
a declaration.
in the case of a recursive definition, we always
find ourselves within the body of the definition
when we encounter the error of not being able to
lookup the name, given we are currently defining
said name, however we can imagine solid semantics
of recursive names in both data structures, namely
recursive references. and within algorithms, namely
recursive calls. we can imagine solid semantics for
these things within the language. namely, a pointer
which has the same referent type as the type of the
containing structure, and this is probably a list,
or tree, or graph.
or a recursive algorithm, which, can be supported naturally
in a stack based execution environment.

this is no longer possible due to rewriting all of
the parts. we solve this problem differently now.
types are explicit within the grammar, and are not
composed of a kind of binop, although actually,
with the inclusion of a type literal entity, we can
redefine type operators as a affix expression.
(meaning we use the operator precedence parser
 to parse type expressions. the very same procedure
 which parses all other affix expressions.)

----------------------------------------------------------
in an attempt to give some coarse language,
i find language which varies alongside the
varying levels of scale in any given problem
introduces a lot of the logical footholds needed
to talk about things within that scale but also
between scales, to the abstractions which are daily
used in programming, we must first define something
basic, scales themselves!
given a program, essentially we talk about abstractions
which operate at some scale. composing together
abstractions which operate at different scales to
compose new abstractions. this may sound weird but
its just a useful conceptual shorthand really.

scale scale:
      at the bottom: nothing/zero/nada/nil
      i.e. we are talking at the scale of nothing.

      then:   one thing
      i.e. we are talking at the scale of one thing.

              n things
      i.e. we are talking at the scale of some fixed
           and known number of things.

              f(n) things
      i.e. we are talking at the scale of some
           computable number of things.

              countably infinite things
      i.e. we are talking at the scale of a
           countably infinite number of things.

              uncountably infinite things
      i.e. we are talking at the scale of an
          uncountably infinite number of things.

      (note: there are so many kinds of infinity
       I just use the major distinction here because
       this is again, a shorthand)


in a program, variables act as names for things.
sometimes they act as a name for nothing,
sometimes they act as a name for one thing,
sometimes they act as a name for a collection of things,
sometimes they act as a name for an infinite number of things.
variables are thusly, scale independent.

procedure application sites are highly scale
dependant, where each call site needs both
a specific n things, and those things need to
be specific things (have some specific type).



----------------------------------------------------------


#! binops and currying/closures

a binop is a two argument procedure.
so, we need a set of binary procedures
against which we can lookup the op,
to retrieve a procedure, in the same
way that we name regular procedures and
can lookup the procedure to find
it's definition.

this raises an issue however,
given that procedures only take a single
argument, how do we propose to define
binary operators in terms of single argument
procedures?

add := \x => \y => x + y

or, in the presence of tuples
add := \(x, y) => x + y
this means implementing tuples.
i mean, it goes a long way toward
implementing structs/records.

or, if we define procedures such that
they can take lists of arguments
add := \x, y => x + y

which means thinking about multiple dispatch.
because we were previously thinking about
single dispatch only.

if we define it by way of currying,
we can use single dispatch only, as each
PolyLambda object is itself a dispatch
procedure, and each of it's monomorphs
are what is dispatched too.
when interpreting we would be able to
take advantage of the fact that partially
applying an operator can instead return a closure
in the exact way that partially applying a
procedure allows, precisely because
operators are implemented as procedures
under the hood.

however to my mind, having CLOS style multiple
dispatch, is in the end the cleaner solution,
because of it's fuller solution to the
expression problem, whereas the curried style
solution sticks us squarely in the extending
behavior is easy extending state is hard
camp of the expression problem. right with
other static functional languages.


to me, it would be nice if operators were
identical to procedures, it just so happens
that their name must be made up of a subset
of characters, and they have a precedence
and associativity associated with their
definition.

so, we need a symboltable
which is specifically for operators?
i mean, there isn't any reason why
we couldn't use the same table.
other than the binop definition
object not inheriting from Ast.

we would need to store a binops definition
along with it's precedence and associativity.
 would this have implications
on the ast structure storing binops?
i feel like we can have a separate
definition structure. but we can store
it in the same table, and use the same string
lookup.
 does this imply that the op-prec parser
 looks up the entire binop definition?


and do we provide some sort of
half-applied procedure object, which
would support partial application of
both procedures and operators?

by partial application, i really mean
currying. the order of arguments matters,
as conceptually, multiple arguments
are a tuple. if we call a procedure
with less than the normal number of
arguments, (keeping in mind order matters)
we can store the resulting
object as a closure, with the previously
given set of arguments stored somehow
and passed into the procedure via the
associated parameters, with the unfilled
arguments making up the argument list
of the resulting object. if that makes sense.




inc := add 1

in the interpretive environment, with
the currect execution schema, we
support closures naturally, as the tree
itself gets the closed over information
spliced in as a local copy
inc ==>> \y => (1) + y

but this makes no sense from an assembly
perspective, as there is no tree to
dynamically express the procedure.
instead we have a procedure body
represented in assembly. and each
individual argument represented in
assembly.
a partially applied procedure seems
to require a possibly infinite number
of procedure definitions, one for each
specific argument value. however,
if instead we use the same procedure
body, and instead store precisely the
curried arguments until such time as the
entire argument list is provided.
essentially, delaying the procedure application
until we are provided the final argument.
we can define this procedure 'inc'
as a call to the procedure add, where
the first argument is always the value
one. the compiler can thus choose to
construct a new procedure body, with
the single variable of some type, replaced
by a literal value in the same location
(according to however that value is stored
 when passed as an argument.)
or, we can utilize the exact same procedure
body, and implement every application of
the inc procedure as precisely a call to
add with the first argument filled in by
the literal value 1.
this seems to follow for any given type that
can be stored as a local value. we can store
the 'saved' value in memory, and store the
closure as a procedure address and a pointer
to the closed over values. then when the
compiler wants to emit a procedure call
it can use the closure values to fill in
the missing arguments, use the call site
provided arguments to fill in the rest of
the argument list, and invoke the procedure.
inc  ==>> \(x : Int = 1, y) => x + y

inc 2 ==>> \(x = 1, y = 2) => 1 + 2
      => 3

in effect, inc is a wrapper around a
call to add, where the first argument
is bound to the definition of the wrapper
within an object I called the closure, (formally
the closure is this data in addition to a pointer to
the function.)
if the closed over name is a variable and
not a constant, then the closure will need
to live at least on the stack, to be
available for modification, per instance of the
call to the closure. if this wrapper/closure is returned
by some callee, then it's closure must live in
the heap, and be deallocated when we no longer
have a refrence to the closure object. (it's name
falls out of scope)



#! refrences

this is the best example I can think of,
to start this discussion.
when we consider a type reference as a subclass
of the referent type. the relationship between
reference type, and referent type, is to say
that a reference type is-a referent type.
T is-a T^
we can extract values from a reference type
with the same symbol (the dot operator)
as we would extract from
the referent type. (this saves the programmer
from having to explicitly direct, or indirect
a variable or pointer value syntactically.
with the compiler essentially allowed to infer
a single layer of indirection with any name.)

so, it is not technically accurate to say that
references are equivalent to their referent type,
as the computer must preform a different sequence
of instructions to extract a subfield of the
referent type, that from a refrence type. (the
compiler must wrap or unwrap the type with
a step of computation essentially.)
this is a pattern with the optional type as well.
we want to simultaneously think of the name
as being
A) implicitly convertable to a boolean,
  with true implying that the optional is filled
  with some type T
B) implicitly convertable to a T, and
  when we use an optional name as we would
  some T, we can apply those operations
  directly to the storage representing said
  T.

in this way, we want the syntax around using
refrences to be as unobtrusive as possible.
because having to be exactly explicit and correct
about each particular low-level step to verify
constraints becomes tedious, error-prone, and
gets in the way of putting ideas into reality.

we can think of the reference type as being a
supertype of every type, because everything that
is typeable has an address. as that is precisely
how the compiler associates syntax with memory.

in the same way as references, we can validly imagine
wrapping any type in an optional type. and thusly
the name is both a optional type and a regular type T.
simultaneously. this seems to imply that is-a
relationship to me.
these two things feel analagous to functional monad
abstractions

#! monads
the monad is a design pattern
that follows three rules.

left identity:
  a value of some type T wrapped in a monad,
  passed as an argument to some function f
  taking a monad containing said type T
  and returning a monad containing the
  same type T or another type T',

  is equivalent to passing the type
  T in a call to a function f taking
  some type T and returning the same
  type T or another type T'.

  to say this another way, if you
  have two procedures, with identical
  bodies, except for the fact that
  one of them unwraps the type before
  performing the action. then these
  procedures are left identical.

  the type is predictably the same, just
  wrapped or unwrapped.

  right identity:
    the monad containing some type T,
    when called against some function
    f which returns the monad bound to
    it's argument, that same type T,
    it is equivalent to the identity
    operation. which is to say, a no-op.

  associativity.
  if the monad type is associative,
  then this statement holds:
  if you apply function f to the monad
  containing type T, and subsequently
  apply the resulting monad to the
  function g,
  that is the same as applying the
  function g to the monad containing
  type T, and subsequently applying
  the resulting monad to the function
  f.

a monad is three things:
-- generic type M
such that we can store
a type T within the M.
  M(T)

-- the function wrap
  with-type: T -> M(T)
essentially, a constructor.

-- the operation to apply some procedure
    to the contained value, returning a
    monad. allowing for chaining of actions.
combinator/binary function bind
  with-type: M(T) -> (T -> M(T')) -> M(T')

essentially, unwap the T,
apply procedure to the T,
return the result of the procedure
  wrapped in a monad. (the wrapped
  type T can change, in the same way
  that a procedure can take some
  T and produce T')


to be a full mathematical monad
wrap needs to be the left-identity of bind
wrap needs to be the right-identity of bind
and the combinator needs to be associative

every unary operation valid upon some type T
is a valid operation to bind to the monad
storing type T.

we could imagine a unique_ptr<T>
as representing the semantics of owning
references in pink.
with the assumption that
the destructor of every name that isn't
returned is called at the end of
the defining scope of the name.

we have references, they are valid upon
every type T, we can imagine having
a type T stored in any reference.
as the monad wrap or unwrapped states
could correspond with
taking the address,
and retrieving the type.

wrap/take-reference &T
(packages type in a reference)
T -> R(T)
bind could be the procedure which
unwraps the bound value/follow the indirection to the data.
and applies procedure f upon it.
R(T) -> (T -> R(T')) -> R(T')

we don't usually think of references
in this way, they are usually used
like a unique type within the type
hierarchy. but if we consider them
like this, do we gain any interesting
properties?

we could imagine single argument
procedures, which operate upon
type T's, we can always bind these
to monads holding type T's.






  (var: type (',' var: type)*) -> type

  okay, here's a thought, all of this is great.
  and useful and on the right track. but at this
  point, it is starting to become too big to tackle
  all at once. so, how about for this early version
  functions are only one argument. and we put a pin
  in true multiple dispatch. well then, we still have
  a problem, what the heck are binops?


  this argument list becomes essential, and
  the method with which we dispatch and
  disambiguate will more than likely create the
  particular peculiarities that live in
  language standards.

  fn proc (a: int, b: int) => ...
  fn proc (a: int, b: double) => ...
  fn proc (a: double, b: int) => ...
  fn proc (a: double, b: double) => ...

  then application sites can look like:
  proc 1 2
  proc 1 3.2
  proc 3.14 5
  proc 2.1 5.6

  which will dispatch to the most specific member
  of the procedure set.

  essentially, a procedures name has a single zero argument
  possible overload.
  a set of single argument overloads
  a set of two argument overloads
  a set of three argument overloads
  and so on to
  a set of n argument overloads

  if there isn't a user defined
  overload of some number of arguments
  then the compiler cannot infer a body.
  so, there isn't a single dispatch method
  per procedure, which covers all the cases,
  there is a single dispatch method per set of
  overloads, and the compiler selects which one
  based on the shape of the call site.
  maybe that sounds a little weak actually.
  each call site is represented within the
  syntax as some finite number of arguments,
  so doesn't it make sense to rely on the number?
  what about variable lengths of arguments?
  to support a printf style structure? well,
  that's really just a standardized vector
  of the arguments as the last argument to printf.
  called va_start. meaning we can provide a wrapper
  around the c function once our language supports
  vectors.

  how do we handle overloading the number of arguments
  with polymorphism? well, i suppose that a poly
  argument is something which can act like a base
  <object> argument type for a given procedure.
  meaning, if you provide any given number of arguments,
  as long as there is an overload set which has the same
  number of arguments as are provided, and each argument
  type either matches, or when we find a poly formal
  argument, we know we need to generate a new concrete body
  with the matching type, and the body is assumed to be
  the body of the procedure with the same number of
  arguments, not just any body appearing within the overload
  set. during the interpretive phase, this dispatch is
  carried out on the fly, but when we are compiling code
  there are sublty different constraints. this selection
  algorithm needs to be cognizant of both use cases.
  but that puts the cart before the horse information wise.

  proc (_:<type>, _:<poly>)

  single dispatch over the runtime type of the second
  arguments sure,
  and also, each application site tells the compiler to infer
  another concrete instance with the argument bound to
  some specific type. this then becomes the concrete instance
  which is dispatched too.

  proc (_:<type1>, _:<type2>)

  now, this is a user specified overload of the
  instance within the overload set. now instead
  of the compiler inferring a new body when it
  encounters an application site providing the
  corresponding types.

  this then makes it cloudy what a funtion pointer
  even means. would i have a function pointer to
  each overload set. truly, the argument list seems to
  be a tuple.


  so, instead of multiple dispatch procedures, we need to
  contend with nesting the dispatch I think.
  so, the outermost switch could be on the number
  of arguments. and then each sub-switch would
  be based on the type of the first argument. then
  another switch with the type of the second argument, and so on.
  the only time we consider code generation is precisely
  when arguments have polymorphic type. in this case only,
  when the dispatch procedure fails to find a suitable
  alternative procedure body with a matching type the
  compiler is allowed to generate a new body, (a copy of
  the body associated with this polymorphic parameter)
  (if the language spec allowed for on-the-fly code generation
  one could imagine being able to provide this polymorphic
  code generation capability all the time, such that any new
  type could work when passed into a procedure with a body
  that was applicable to the new type.)

  of course all of this dispatching is only over exactly
  which procedure bodies are provided explicitly by the
  programmer.
  so, if the programmer defines a procedure such as:

  fn proc (_:<type1>)
  fn proc (_:<type2>)
  fn proc (_:<type3>, _:<type1>)
  fn proc (_:<type3>, _:<type2>)

  the switch (representing the dispatch procedure) could be
  switch (actual_args.length()) {
  case 1: {
    switch(actual_args[0].type) {
      case type1:
        ...
      case type2:
        ...
      default:
        report error

    }
  }
  case 2: {
    switch(actual_args[0].type) {
        case type3: {
          switch(actual_args[1].type) {
            case type1:
              ...
            case type2:
              ...
            default:
              report error
          }
        }
        default:
          report error
    }
  }

  }

the inner switches can be generated by composing
the choices posed by the second variable within
each possible choice of the first variable.
and so on for the choices of the third variable
inside each choice for the second, and so on.
(most use comes from 2, 3, and 4 argument procedures,
if metrics from
other languages implementing multiple dispatch are
to be believed, and given my inclination to refactor
the getype and evaluate procedures to take a structure of
bindings instead of passing each as an argument makes
me believe this. (i.e. i don't want to write and
maintain procedures of more than a handful of
variables. if your function is operating on
that many things at the same time, your design is
bad. and you need to take a serious look at how
your data structures are designed, and rethink them.))
functions like substitute which operate over four
variables are reaching the upper limit.

----------------------------------------------------------------------

# polymorphism the usual way:

so, instead of what i am currently doing,
the formal treatment of polymorphism travels
down a different logical path.

we ask the question:

given the context ? and some polymorphic term t
can we find a type T for t such that t
typechecks?

given the environment (ENV, usually written '?')
and some valid term in the language the
algorithm generates a set of constraint equations
that must be satisfied by any type substituted into
the body. these equations themselves are the usual
constraints of the typechecking algorithm, and so
this algorithm looks similar to the regular typechecking
algorithm. however, instead of checking the constraints
we record them when they appear, simply making note of
what each type is used for within the term.

this constraint set is then 'unified' which reduces
the cosntraint set to it's smallest possible equivalent
representation, while also double checking that
the constraint set is not internally contradictory,
as in, for no type we could give does the body typecheck.

when it comes time to apply a polymorphic procedure we
substitute the formal types with the actual types provided
by the application, and then we check that each type
constraint is satisfied by the actual provided type.

formally, we do one step between the initial recording of
constraints and procedure application, first we 'Unify'
the constraint set. which is another algorithm whose
purpose is to ensure that the constraint set generated by
the first algorithm is
  a) solvable at all (that is, given any type)
  b) reduces the size of the constraint set to be
      the so-called Principle Unifier. which is
      essentially the smallest set of constraints which
      fully types the term in the language.


given a grammar that includes

term := x
      | nat
      | true
      | false
      | term term
      | \x (:Type)? => term

with typing judgements:

----------------
nat : Nat |- ?

----------------
true : Bool |- ?

----------------
false : Bool |- ?


  x : T |- ?
---------------
  ? |- x : T


  x : T1, t2 : T2 |- ?
-------------------------
? |- (\x:T1 => t2) : T1 -> T2


  t1 : T1 -> T3, t2 : T2 |- ?
------------------------------
      ? |- (t1 t2) : T3



to typecheck the term
  'let' x := t1 in t2
in the context ENV looks like:

1) generate the set of constraints,
    regarding the usage of t1 within
    the term t2. we call this C

    the constraint typing rules are just that,
    a typing judgement over the shapes of
    term in the language such that given a tree of
    terms the algorithm returns the set of typing
    constraints that must be satisfied within the term.
    (if we use the argument as a procedure then the type
    substituted in for that argument must have procedure
    type, and other such constraints are what we store in
    this set.)

    x : T |- ? |x C
-------------------
? |- x : T |x C union {}

(when we encounter a variable of some type,
  that adds no constraints to the set)

  x: T1, t2: T2 |- ? |x C
-----------------------------
? |- (\x:T1=>t2) : T1 -> T2 |x C

  (the usual type buildup of a lambda term,
    which maintains the constraints in the set)


  t1: T1 |x1 C1, t2: T2 |x2 C2
  x1 disjoint x2 = {}
  x1 disjoint FreeVariables(T2) = {}
  x2 disjoint FreeVariables(T1) = {}
  X is-not-a-member-of x1, x2, T1, T2, C1, C2, ?, t1, t2
  C' = C1 union C2 union {T1 = T2 -> X}
------------------------------------------
  ? |- t1 t2 : X |x1 union x2 union {X} C'

  (when the conditions are satisfied this judgement
  adds the constraint that T1 must have procedure type,
  whose argument type is T2 and whose return type is X)

  (if we consider the judgement for the conditional,
  when we get past the unique typenames convention,
  the constraints being added are that the type of
  the conditional must be Bool, and that the types
  of the fst and snd alternatives must be identical.
  which is the exact constraint that we check for in
  the loop as well, except we just require the body
  have a type.)

  so building up the constraint set is rather straightforward,
  we consider each typing judgement, and instead of testing for
  the type's being correct at that moment, instead we record
  each test the typechecker would have done to each argument
  in a list.
  then later, when we encounter a situation where we need to use
  the polymorphic term, we instance the type variable with some
  monomorphic type and see if the constraint set can be typed
  given the monomorphic type.(this is where the logic of HasInstance
  lives right now, the 'constraint set' is never calculated, instead
  we let the usual typeing algorithm test the constraints after the
  fact that we have been given some concrete type. this doesn't give
  as much information however (we don't have enough type information to
  make the same judgements in the case of references),
  and is not as flexible as this method, which allows for polymorphic
  records as well (with some difficult adjusment).)
  (the sorts of constraints are just the ones from the judgements,
  and the other semantic units, like sequence simply union the
  constraint sets of the terms it holds.)


2) unify the constraint set C to both
  a) obtain the principle type
    (a most general solution to the
    constraint set above, call that T1)
    this is essentially the smallest set
    of constraints which fully satisfy the
    typeing of the well, types (each 'type'
    is actually a type variable which substitution
    is going to be applied to fill in each
    type variable with a concrete type later.)
  b) ensure that the term -has- a principle
      type (that is, that the constraint set
      is even solvable. i.e. that the type is
      used in a way that has a solution given the
      types available within the program.)
      (this allows us to report an error in the
      usage of terms even though we lack a specific
      type.)
  this unification also happens to leave us with the
  smallest possible set of constraints which fully
  type the term.

3) construct the type scheme for the term's Type,
    (the literature calls this generalizing the variables?)
    this essentially corresponds to the list of variables
    which must be given a concrete type within the body of
    the term. the arguments to each procedure within the
    body that are polymorphic.

4) each time we encounter the polymorphic term we must
    be able to use the type scheme in a typing context.

so, something concrete:

the term

  \x: Int -> Int => \y: Int => x y

(taking from the intuitions sketched by the algorithm above)
is well typed because
1) the leftmost term in the application
    has procedure type. this is a constraint that
    is introduced by the application semantics
    themselves.
2) the type of the rhs in the application,
    the 'actual' argument, has a type (Int)
    which is equivalent in form to the
    expected type (Int) of the procedure,
    the 'formal' argument.

if we consider the polymorphic equivalent

  \x => \y => x y

we can interpret this term as having polymorphic type

  \x : X1 => \y : X2 => x y

we then generate a constraint set by walking the
tree just like typing and evaluation,
the application adds two constraints to the set (at least)
  X1 must have procedure type
  X2 must be equivalent to the
      formal argument type of X1
      (once X1 has some concrete
      argument type of course)

so the principle type of this term
would be this constraint set
  { X1 = T1 -> T2, X2 = T1 }

when we want to apply a procedure of this type
we first check that type actual argument types
match the constraints the set places on them.

further thinking:
-----------------------------------------------------------------
  \f => \x => \y => if f x y then x else y

  \f:X1 => \x:X2 => \x:X3 => if f x y then x else y

  C = { X1 = T1 -> T2 -> Bool,
        X2 = T1,
        X3 = T2,
        X2 = X3 }

-----------------------------------------------------------------
  contradictory constraints can be generated:
  these should result in non-typability.




-----------------------------------------------------------------
  \x => \y => x + y

  \x:X1 => \y:X2 => x + y

 C = { (+).HasEliminator(X1, X2) }

  (countable? addable?
    valid when applied to the
    binop? call "Binop.HasEliminator"?)






so, this is pushing my brain straight into multiple argument
procedures. passing a number of arguments into a procedure
determines the selector to call. this is because the number
of arguments must be known at the application site.
(variadic procedures cannot be polymorphic as well)
then, each polymorphic argument has a set of monomorphic types
it is associated with. why is this coming from the discussion
of polymorphism above? because each term, at the scale of the global
scope, is associated with the set of arguments which need to be
specialized. which is precisely because currying is equivalent
in semantics to multiple argument procedures plus partial application
support (for free, because of the tree nature of execution).
so, if for any given procedure definition (recall that procedures
are the only language element that can introduce polymorphic bindings,
eventually this will be extended to type compositions)
we must track a set of variables to be specialized, well
then we are doing the work of tracking a particular set of procedure
arguments (types, specifically), but generally we must perform essentially
the same work to support multiple argument procedures.
each lambda is a particular member of a polymorphic lambda.

so lets think through what it takes to type an application
of a monomorphic multiple argument lambda.

  say

 f :=  \x:Int, y:Int => x + y

  then, when we encounter some application

  f a b

  we lookup the procedures argument list
  compare the size of the formal argument list
  to the size of the actual argument list.

  then if those match we compare each member of
  of the set piece-wise until ether we find
  a type that doesn't match or we find a valid
  procedure application. (in the typing case we
  return the procedures return type, the evaluation
  case then immediately applies the procedure.)

  so, if we consider a polymorphic procedure as the
  set of procedures whose shape is the same, but whose
  types can be replaced for any who match the constraints
  presented by the shape (body) of the procedure, then
  we can consider the typing of a polymorphic procedure
  application as dispatching to the member of the polymorphic
  procedure set whose number and type of each argument matches
  the actual argument list presented by the application.
  (if we fail to find a member of the set of sets who match,
   then we can fall back onto generating a new member of the
   set of sets seeing if this new member type-checks given the
   constraints posed by the body.)

  we need to essentially maintain a set of sets, where we first
  separate by, number of arguments (i think is formally called a
  procedures rank? i.e. rank one has one arg, rank 2 has 2 arg,
  and so on.), then each argument is represented
  by a set, where the first position in the set, from argument
  1 to N, is one valid application. so to dispatch to the first
  instance the type of each actual argument must match the type
  of the formal argument in the first position of each argument.
  if one argument doesn't match, we push the index up by one and
  check that set of argument types. (this must be why CLOS sorts this
  set on each insertion. we can store in sorted order if we use a linked
  list implementation. (or a custom class over dynamic arrays.))
  when we fail to find a valid instance, if the method is polymorphic
  we can attempt to type a 'new' instance whose formal argument
  list is equivalent to the actual argument list.

  so, now to think about sorting.

  each monomorphic type is considered distinct.

  so the types

  {A, B, C}

  allow for a single procedure to take on:
  A -> B
  B -> C
  C -> A

  xor
  A -> A
  B -> A
  C -> A
  (this could be an A constructor.)
  as valid sets.

  and

  A -> A -> A
  A -> B -> A
  A -> C -> A
  B -> A -> A
  B -> B -> A
  B -> C -> A
  C -> A -> A
  C -> B -> A
  C -> C -> A

  this can be called by passing in
  any two of A, B, C, and in every case
  we return an A.
  each of these procedures would need to
  be represented in assembly. and the
  argument type and order matter to the
  assembly, which is why each of these
  need to be distinct.
  and why we don't
  allow the procedure with the definition
  A -> B -> C
  be called by passing in a
  B and then an A.

  we allow polymorphism of the type, number and order
  of the arguments.

  (we still can't allow polymorphism of the return type,
   because we can't distinguish if we want
   A -> B
   or
   A -> A
   at the application site, the information needed to make that
   choice is almost always in the head of the programmer.)


a monomorphic lambda can be supported by
the current class,
a polymorphic lambda needs to be distinguished
in two ways, the kind where the text of the body
is meant to be taken by the compiler as a
template for type substitution, and the kind where
we are just adding alternatives to a set of overloads.
I would also like the solution to be robust enough to
allow the programmer to provide their own instances of
some given polymorphic procedure for a template,
which allows for the programmer to provide a different
body for a given type, if the body that would be generated
by the template would be untypable, but you still wanted
to provide the procedure for the given type.
(c++ calls this partial template specialization.)
so, how about, lambdas are allows to be fully monomorphic
or fully polymorphic. no overloading.
whereas, there is now two different kinds of function
declaration, monomorphic/overloaded or
polymorphic.

fn A '(' <argument-list> ')' <body>
fn A '(' <argument-list>' ')' <body>'

the interpreter will dispatch to the different
bodies based on the types of the provided arguments.
we allow the procedure to have the same name
and dispatch based upon the runtime type(s)
of the argument(s), we allow a set of procedures
each with differing argument lists to share the same
name. this is a monomorphic overload set.
(type after evaluation, in the runtime this would
be the value after evaluation.)

polyfn A '(' <argument-list> ')' <body>
polyfn A '(' <argument-list>' ')' <body>'

we allow a polymorphic procedure to be described by
a single template body per rank of the procedure.
a single argument polyfn A set, a two argument polyfn set.
three arg and so on. (maybe?)
then we can allow the programmer to provide a new
polyfn A definition, however if they provide a single argument
of monomorphic type then we can specialize this
procedure over that type.
so, a template has a rank, and if we consider a call to that
procedure with less than the required number of types we construct
a closure and delay the actual application for a later time.
just like with a monomorphic procedure application.

however, what about overloads/specializations which provide a call
signature that differs in the number of arguments?
essentially, defining the specialization/overload takes the
place of the compiler ever being able to infer a closure creation,
should the types overlap. so, do we even really want partial application?
maybe we remove that feature in favor of letting the programmer explicitly
delay application by way of procedure pointers.
we could even maybe consider a polymorphic class which implements
the 'closure' semantics within the language itself. (over
a template parameter meaning the procedure pointer name and
a dynamic array of values. (since values know their type they can
be positionally applied to the procedure pointed to by the pointer.))
such that a programmer can construct closures when they want them
explicitly, so if their existence precludes some design pattern
the language would still be able to express them.
(partial application i think.)



   additionally:
   the parser needs to be modified to accept
   lambda definitions with multiple arguments,
   and the lambda definition needs to be modified
   to have an argument list instead of a single
   argument, and the application typeing and evaluation
   judgements need to be modified to compare argument lists.
   also the HasInstance procedure needs to be modified
   to accept and do it's work over argument lists.
   also, we then need to add support for partial application.
   (construction of an unbound closure
      around the appropriate lambda or polylambda,
      which the application typing and evaluation rules
      know how to deal with?)

ohmygoshohmygoshohmygoshohmygosh i think it's working???!@?!@?!?!?@?@/!@?!

i think i can define composite binary operators now!
and Unops too!
then onwards to a procedure that can read in declarations.

--------------------------------------------------------------------

okay, so additionally, do we want references to be able to be
considered as pointers, and make assignment work on both any
two T instead of a ref T on the lhs and a T on the rhs.
then the programmer can make the subtle distinction
between assignment of the reference, or assignment of the
value pointed to by said reference.
yes, this is a necessary feature of a low level language.
I gotta say, "this series/segment/cluster of memory addresses is mapped out
thusly", and then "write/read such and such value from such and such location"



















------------------------------------------------------------------------
