



if we consider optimization to be valid then we must
consider it replacement of terms across and along some lines
in which behavior is equivalent and either time or space
is compressed compared to some original implementation.

essentially, the optimized version should behave identically
to the not optimized version, but it should do it in less time
or space.

so, the test rig of this language could take this into
consideration maybe? say we replace the tree style evaluation
with continuation passing style? or an instruction interpreter?
which is faster given identical language definitions?










why rewrite in c++?

-- future me: watch past me discover that i was using
              the polymorphism wrong, and subsequently
              stumbling upon the classical interpreter
              pattern.

well, the parametric polymorphism is attractive,
but it requires so much syntax and is only
available on a single argument anyways.
which needs to be set up in the sideways way
of defining each alternative to the polymorphic
argument as a subclass. this makes it feel like
it takes at least as much work setting up the
feature to then use it only in a restricted
fashion. and c++ isn't expressive enough to
factor the handrolled dispatch code
for typechecking and evaluation that i thought
having some amount of polymorphism would
write for me. but alas, this is not even close
to the case. to make use of polymorphic dispatch
of the typechecking algorithm such that i could
call a single typeof procedure and retrieve the
type annotation describing the term without having
to write the dispatch over the Ast term type
myself, would factor each typeing judgement into the
Ast class itself. then I could say,
"term, what is your type?"
or
"term, what is your value?"

this distributes the algorithm throughout the Ast class itself,
increasing the complexity of each Ast node considerably.
in fact, each individual node would need to contain both
the typeing and evaluation rules within it's body.

the reason i did not go for this implementation first is that it
seems to break a rule of OOP, which is separation of concerns.
it makes sense to group the typeing judgements into a single
class, such that we can consider the typeing algorithm as a whole.
but this forces the dispatch code into code i have to write.
which means it's code i have to maintain, which means it's a
possible bug each time we extend the program.
with the distributed version we instead have to provide the typeing
judgements associated with the class it is typeing, and homomorphically
/similarly with the value jugements (evaluation rules)
this however raises an interesting question, where is the symbol table
association stored? both types and values are dependant upon context.
hint: it is now a parameter to the procedures.
so now, instead of there being a separation of concerns whereby
we separate the via the distinct actions we can take on entities,
and instead we separate out our concerns by entity.
in the end, why not now reconsider the program.
how about, the parser is now a producer to the
typechecker and evaluation which act as consumers of the
terms produced by the parser. (homomorphically for the
translator unit.)
the typechecker has a signature like
  ast-term, env -> type-signature
the evaluator has a signature like
  ast-term, env -> ast-term
and we have a pair of string conversion
procedures which allows turning a string into an
ast-term and back to a string.
  ast-term -> string
  string -> ast-term
and we say that the programming language is a collection
of entities, such that each entity has this set of
procedures defined upon it.

and we restructure the program such that each entity is
a pair of files, and we have a 'entities' header which
includes each of them so that other files can include
everything easily. we still will have other concerns
in other files. (symboltable, error, parser, and lexer
to start.)

oh, my, god. apparently this is the standard interpreter
design pattern...

well, at least I rediscovered something of quality.

it's just, with all the programming languages
books I have read, how did I miss this pattern?



okay, following this pattern, plus the *Judgement pattern
has added so many more files than the other versions
of this project. that isn't really that bad, as it has
sort of alleviated the problem of the extra long files
like the c version of the Ast structure, which is a
1000 lines on it's own. or the c version of the parser,
typechecker, and evaluator, each over 1500 lines.
instead the most cluttered file in the project is
Entity, which is currently encapsulating the
entire primitive object herarchy/representation (future me: not anymore!)
as well as declaring the Ast node which holds Objects.
(Entities themselves.) ((Object and the Object hierarchy
are going to be split into their own source files
however this is going to happen after i cleanup the
source code folder itself to add some more folders
and then i have to edit each include to have the right
path, which is going to be a pain across 58+ files.))
and both files are less than 200 lines each.
though, it hasn't really made it less code. there is
still a sizeable amount of code in each file.
(note: in each case the source of the lexer is around
~1500 lines, but this is generated from a ~120 line source file.)

the old c source works at 23 files, more than five files approaching/at 1000 lines
the old cpp source i abandoned at 27 files, some files approaching 1000 lines
the new cpp source is at 58 files, no individual file breaking 200 lines.
  and the majority of the files cannot be scrolled at 100% view.
  i will say that the PinkKernel file, and the Entity file are both
  places where we can expect much more growth before the language is
  complete.

the thing that really brought the source code file size down was breaking
up the large 'stages' into their individual components spread across
each nodes defining file. of course, the same semantics need to exist
so this hasn't reduced the code size overall, as now there are many more
files with smaller files sizes. this is in fact more organized than
before, and I spend less time searching the codebase
 this in fact has also had the effect of localizing
the complexity of each of the components of the stages, as the
nodes stage code need focus only on what is important about the node.

also this is probably the most cannonical interpretation of things,
and as such is probably very unoptimized. but that comes after it works.

the other thing i have noticed is that most compilers keep going
after the first error looking for more errors. so, to acheive this
we first need to modify the data structure of each *Judgement.
to hold a list of errors, or the success. then we can have the failure
state continued to pass through the algorithm gathering errors
until we reach some maximum. (which given the recursive nature of
programming, errors upstream tend to multiply into errors downstream.)
then we can either print the success and continue, or print the errors
and continue.
this is all probably adding more checks to the various
if conditions in the algorithm to either pass the errors
onwards or when the maximum is reached (or the term ends)
we return the results. (either good or bad. for the control
segment of the algorithm to either report the failures and
continue interpretation, or end compilation; or continue to
evaluation and then continue to more interpretation, or
continue the compilation until and object file is produced,
or a list of errors is produced, perhaps from the linker.)
