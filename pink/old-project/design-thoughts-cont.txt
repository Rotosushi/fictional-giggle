
syntax (form)

semantics (meaning)

types:

type system:
the type system of a programming language is 
it's enforcement of type rules. mainly type systems are
concerned with:

type equivalence:
	when can we say that two types are equivalent?

	there are two kinds of type equivalence:
	structural equivalence, which is based on the 
	content of type definitions.
	name equivalence: which is based on the lexical
	occurances of the type definition.

	structural equivalence between two types
	refers to the underlying composition of the
	two types. in the simplest case we can consider
	int and real.
	in both cases, pink specifies that the size of
	these two types is equivalent to the size
	of a single machine word. so structurally speaking
	these two types are equivalent. their names are different
	so they do not have name equivalence. 
	to cast between the two one only needs to treat
	the machine word as the different type.
	any type that shares structural equivalence can have it's
	cast function written by the compiler. because we know
	how to write an int into an real, a real into a int, 
	int into text, bools into ints. all the compiler must do is
	a deep copy, and all deep copies share the same form, 
	(depending of course on the shape of the types.) which means
	the compiler can generate deep copies as well.
	if we consider the case of int -> real we can garuntee the
	cast will not change the physical value of the underlying representation.
	we will have to change the state however, because the interpretation
	of the bits is what matters. all such casts, where types have a one-to-one
	mapping of their value sets from the one to the other can be considered 'safe'
	in this way. 
	however, we will not follow in c's footsteps here. I feel that implicit
	type coercion is not the right feature to rely on here.
	these 'safe' coercions are mostly fine in C, for sure.
	however they still have the ability to cause subtle bugs, especially
	the implicit casts of pointers to booleans and integers to pointers.
	if we consider the feature of our language relying on structural equivalence to 
	decide type equivalence only and then we consider the feature of implicit casts between user defined
	types that are considered structurally equivalent; that is just asking to
	magnify the subtle bugs already present with implicit casts, it overall
	weakens the type system just for some syntactic brevety that only has the 
	ability to be fine, or shoot you in the foot. I think that means it's bad.
	however, it would be really nice to remove all the boilerplate code
	of type equivalence and assignment from the langauge. and if we remove
	implicit casts from the language (or rather never add them in the first place.)
	we can ensure that when a user wants to cast between structurally equivalent
	types, they need not write the function themselves, but they can still cast when they need it. (which also automates the task
	of maintaining it's validity in lock-step with the type definition, which lessens
	the maintinence burden of the type. in fact, all types. which is especially nice
	when we consider writing "are these two types equivalent")
	
	important to note however that the user will still need to define their own
	casts between structurally different types.
	and to define value equivalence separately.
	
	
	a definition of structural equivalence will need to apply
	not only to every primitive type in the language, but must
	also extend to the types which we can define in the language.
	we must consider what it means to be structurally equivalent
	for algebraic data types, arrays, tuples, and the data primitives.
	when is a sum equivalent to another sum:

	when is a product equivalent to another product:

	when is an array equivalent to another array:

	when is a tuple equivalent to another tuple:


	name equivalence between two types refers to the
	name of the two types. we say that two types are distinct
	based on the fact that their names are different. this
	might be how type-checking things like function calls happens.
	becuase of the lack of implicit casts. we can consider, does
	the variable or literal passed match the type requested, or
	the set of types requested?

	type aliasing:

	in programming languages with type-definitions
	there is usually the ability to define trivially
	different types. 
	say:
	type farenheit = real;

	where farenheit is being used to represent temperature.
	and temperature is perfectly represented with real numbers,
	so we want to leverage the operations already defined on
	the old type on this new type.
	how much do we want the programmer to have to redefine
	the semantics of real numbers in the type definition of
	farenheit? when in reality any function that can take
	a real, would work when passed in a farenheit. in C, they
	leverage implicit casts, and structural equivalence to
	allow programmers to 'inherit' the semantics of the
	base type. (i say 'inherit' to distinguish it from
	what is programmatically known as inheritance, which is
	a different way of inheriting semantics.)

	so, my first stab at this is to say that if one defines
	a name equivalence through the algebraic data type mechanism
	we would expect the types to be distinct. if we define
	a new type to be the alias of the old type, we could expect
	to 'inherit' the semantics of the old type. to say that
	another way, the new type name would be semantically valid
	in any place where the old type name would be semantically valid.
	so in checking name equivalence of a type during the type checking
	of a function call for instance, we would assume that any
	alias of the typename is also considered equal to the typename, and
	thus a function could be said to be valid or invalid.


	type casting and conversion:
	the processes of casting or converting one type to the other.

	there are three main cases of when we can cast,
	1 - the types can be considered structurally equivalent.
		(int to real, bool to int)
	2 - the types represent two distinct sets of values
		but the values and representation overlap somewhat
		(such as signed and unsigned integers, 
		 or a short and an int, real to int)
	3 - the types are not structurally equivalent, but 
		we can conceive of a semantically meaninful
		conversion function that maps one type to the other.
		(this covers user defined conversions and
		 more complex concrete conversions.
		 converting between cartesian and polar vectors
		 for instance)

	non-converting cast:
		a type cast that doesn't change the underlying
		representation, it merely reinterprets it as a value
		of the new type. one can see this in the case of
		memory allocation strategies. where the heap is itself an array
		of bytes, but portions of the array are reinterpreted
		as bookeeping structures or as user types.
		this is what we call it when we cast between fields
		in a product type (c: union) for instance.

	type promotion:
		type promotion is a particular kind of cast that is done
		under-the-hood to provide non-hardware supported types
		in the language. such as simulating a boolean value with
		a full machine word. or a byte with a word.
		this is completely invisible to the programmer, and
		unoptimized code should not bit-pack these structures.
		whereas optimized code can, and strongly optimized code
		should.
	

type compatibility:
	when can we say that a particular usage of a type
	is valid?

	when I call a function on a name of a certain type,
	is that function call valid?

	when an expression is used as a conditional, is it's result
	a boolean?

	when I return an expression, does it's type match the return
	type of the function I am returning from?


type inference:
	how do we deduce the type of an expression?
	from it's contents and surrounding context.

	each variable used in an expression must be preceded by 
	a defining occurance. (out of order declarations are a planned
	feature, the first use in a context where it has unambiguous type,
	could be considered as it's defining occurance, but that
	is afer we have actually implemented a little bit of the langauge
	and know when that makes sense.)


polymorphism:
	abstraction over types,
	there are two kinds of polymorphism we can consider.

Parametric polymorphism:
	that is polymorphism that exists in the parameter of a function.
	in this case to body of a polymorphic function only relies on
	a certain aspect of any given type, so any type which has that
	aspect can be correctly passed through the polymorphic parameter.
	this kind of polymorphism is generally implemented at compile time.
	where the polymorphic definition acts as a template for function
	definitions, and a static definition for the function can be defined
	for every type that is used with that function.

subtype polymorphism:
	whereby the code works with values of some base/root type, and new
	type can be defined which extend the semantics of the original type.
	but provide the same interface and interface semantics, so the polymorphic
	function can consistently rely on that/those aspects of the type. multiple
	inheritance is less interesting to me, as pink is not object-oriented.
	 however single inheritance can still be usefull when 
	creating and defining certain abstracktions, such as generic containers.

i feel that having type-aspects + parametric polymorphism will do a good
job of covering the case where some base object inherits the behavior
of two orthoginal base classes. we could conceive of a function working
on a set of types, characterized by those types sharing the same semantics.



---------------------------------------------------------------------------

there is really only two kinds of entity that programming languages need to
talk about, and that is single items, and groups of things.

we have collections which we select from which are groups of things.
these collections are either stored staticaly or dynamically and the
collections are structured in different ways. they could be
arrays, records, unions, or pointers.

then we have single items, this is what the other types in the language
are for. describing data with some encoded meaning other than structural.
like a number, a string of characters, or an enum.

we can express single items through their type, and we can express groups
of things through their type. however, the defined actions of a
group is different from that of a single term. luckily we can make 
that distinction using their distinct types.
so for the sake of not having a better name
i am going to call groups of entities; collections

a collection is considered to be an abstract grouping of entities here
	collection are "just" a grouping of entities.
	collections have an assosiated encoding function. (that is, semantics which
		express the set in the hardware or software)
	collections have a type which distinguishes between collections
		and through that type we name collections.

a positional collection would be a collection which adds one rule:
	the members positions are important and must remain constant.
	(they are considered internally ordered by their position.)

each member of the collection shares common characteristics with
it's other members. and it is exactly this common characteristic
which is what defines the collection.

an entity would inherit the properties of it's collection, and 
programmers are free to define new actions and variables
dealing with entities of that collection. (this extends the semantics
that an entity has naturally, without having to inject code anywhere.
you simply write more code which does new and interesting things
with these entities.)

numbers can be characterized as a collection. programming languages need to
talk about numbers with different encodings, well that sounds like multiple
collections of numbers each represented differently in memory according to some process.

characters can be characterized as a collection. programming languages usually
provide aggregate characters as it's own data-type, called a string.
this aggregation process could be characterized as another collection. or multiple
collections depending on the storage mechanism you wanted the collection in, or where
you wanted it.
(contiguous vs list, local vs heap)

booleans can be characterized as a collection. programming languages usually
allow some type which can take on one of the values from this collection
and test it's value to make choices. this is the same semantic action as
a character or a number, it's just according to a different collection, and 
a different encoding.)

(int, real, binary, decimal, octal, hexadecimal, fixed point, etc)
every variant differs in encoding and which numberic values from the real number line 
the collection can represent. each has it's own collection of operations. many are the same
operation, just overloaded.

other things that can be described as a collection
(chars, enums, types, overloading, parametric-polymorphism,
		composite structures, streams, )

enums are a kind of collection where each value that variables of the type can
assume can be reffered to by name in the text of a program.

types refer to both the collection of operations (functions) that can be applied
to the variable, and the collection of values that the variable can assume.
this is true regardless of what the type -is-. it could be composite,
it could be an aggregate, or it could be one of a collection of composite types.
it could be a named type (record) or an anonymous type (tuple),
it could be stored locally or could be stored on the heap an pointed
to by a local pointer variable. the type of something is what we use
to distinguish proper from improper usage of variables as well as what\
functions the user means.

functions are characterised by a few collections, they are typed by their
argument list (a positional collection),
and the body has both a collection of declarations which
comprise it's physical form in the stack of the program, and
a positional collection of statements which comprise the sequence of
instructions which will be run to execute that function. with
items executed in the sequence of their appearance.

overloading defines multiple functions with the same name into a
collection of functions which can be disambugated by the compiler via
argument lists.

parametric-polymorphism is a collection builder notation for overloaded
functions and types.

(in this way, a function which takes a variable as a parameter,
	should only rely on the parameter in the way the collection provides
	a garunteed operation.
	
	like, if your function says it can operate on any member of the collection
	describing all possible representations of numeric values,
	it can only use functions which are defined for every member
	of the collection of numeric representations.

	to use a function which only makes sense on some subcollection of
	the members of that collection could possibly be a runtime error.
	so it should be at the minimum a warning but i think an error 
	for now.
	
	I propose that should the programmer attempt to use a function
	that isn't defined on every member of some collection, then the error
	should indicate that every member of the collection needs to have
	some valid function which performs that operation defined,
	and to then list the entities which do not have the function
	defined. i don't know if this is exactly describing what i mean.
	really, the compiler should be focused on pointing out the 
	points of friction between what you have stated, and what is missing
	to fully implement the underlying semantics.
	)

(programmer specified overloaded instances of a function should be as simple
as defining that function yourself and the compiler will choose that definition
over the one it could construct via the polymorphic definition)

composite structures (named records, tuples, unions) are positionally accessed
collections of data, that can be created and destroyed by the language. a union
is a collection of records that a variable can take on. 

(again, this same theme keeps
repeating, the collection defines which values the type can take on and which encoding
the variables of the collection will use and which operations are defined and which 
are undefined.

there are three states of being for entities in a programming language

what we say : the syntax which describes the entity to the language
				and to other programmers

what we mean : the abstraction we hold in our heads, or write in some meta-material
				that describes the entity

what it is : the implementation details


if we want to enscribe entities into our programming language
we must contend with the needs and desires of all three states.


first, some basic language and ideas.

we could imagine transitions of an entity between these three states,

most entities start out as ideas, or "what we mean".
then we go about encoding that entity into the syntax of
the language, "what we say", the compiler (or interpreter)
takes that syntax and transforms it into "what it is", or
the sequence of instructions that carries out what we said.
it is up to the programmer to ensure that "what it is"
aligns with "what we mean".

the language is there to provide the raw materials to
construct "what we mean" from "what we say". even though,
in the end, the transformation is blind to "what we mean"

aside:
	imperitive : the resulting "program" is in essence
		the emergent property of the execution of the
		instructions.
		(good for very low level programs,
		 or extremely performant programs)

	declarative : describes the emergent program directly
		while hiding more of the implementation details.
		(good for very abstract programs)

the raw materials:

atoms: 
atoms are state described by type
they can be described using
composite structures (records, arrays, unions, bit-packing)
local variables 
local and global constants


functions:
can be described using the three basic forms
if, while, and the sequence of instructions.
the behavior is always in terms of lower understood
	operations. which can be convienently and
	succinctly expressed by affix expressions
assignment semantics by default
new forms can be described using macros.


I/O (program arguments, streams of data)

(a data stream is in some sense an infinite collection,
 a collection without a predetermined number of members
 (where ord(collection) is undefined) must be treated as
 if it had infinite members if we wanted to explore
 the collection's members. (iterate over them)
 and as the creators of Unix noticed, streams
 can be used to characterize every form of I/O in
 a programming language, web sockets, to files, to
 program interoperation and communication.)

 (we can caracterize interacting with data buses using 
 the language of streams very intuitistically.)

 (but interacting with static hardware devices usually requires
 following some protocol before we can read or write data.
 
 and devices which are not primarily characterized by the transfer 
 of data aren't characterized well by streams at a low level.
 
 this is why we need drivers, which transform the hardware interaction
 into a stream interaction, or some other kind of interaction.

 pink is a systems programming language, meaning it purpose is to
 provide the tools to easily create that interaction transformation.

 this is sort of ironic, as the language itself is an interaction
 transformation.
 )

---------------------------------------------------------------------------

control:
	iteration:

	selection:

	recursion:

	non-determinism:

	concurrency:

abstraction:


Binding Time:
a binding is an association between two things.
the binding time is the time at which we create bindings
for things.

(none of these are intended to be complete descriptions of every binding that occurs during the time,
	rather they are to pin down what kinds of descisions that are being made, 
	to better split the language along the time axis.)

Language Design Time - we decide which control structures and primitive types are in the language.
	as with the set of kernel entities. generally things like the type system are also decided here.

Language Implementation Time -  conceptually in this case, this will be bindings
	which are created when pink' outputs a new compiler, or when pink is implemented.
	traditionally, a languages specification does not equal their implementation.
	the specification document is treated as a purely abstract document.
	(existing purely in the realm of "what we mean")
	so there can be a difference in the language when you consider different
	compilers for the language. typically this includes things like the precision
	of the primitive types, the binding of primitive I/O operations to the OS, 
	and the organization and maximum size of the program itself. (it's stack
	and heap. these are subsequently also things which pink' will be concerned with.)

Program Writing Time - programmers make their descisions here. so they are working with
	the abstractions provided to them by someone else, or the abstractions they
	write themselves. writing the low level and high level structure of their
	program in the source text to be analyzed.

Compile Time - compilers map the source code to some semantically equivalent machine code.
	they also lay out the static memory of the program. in here we have most of the stages of the
	compiler, lexing & parsing, semantic analysis, optimization, and translation to assembly.
	we decide what the definitions of things are here, we decide what code we are going to
	include in the static binary and the dynamic binary here (based on programmer requires). 

Link Time - static linking will take external code and insert it into the static definition
	of the program. the linker also	resolves the names accross the separate modules of code.
	virtual addresses are also picked here for variables and functions (maybe physical addresses,
	but only if your OS doesn't support memory paging.) 

Load Time - dynamic linking occurs here, where when the loader runs to take the code from
	where it is stored statically into RAM where it can be run. the dynamic linker also
	runs at this time, and will fetch unresolved external code, and insert it into the
	programs dynamic definition (the program as it exists in memory).

Run Time - run time is from the point at which the program starts, to the point at which
	it ends. run time subsumes program boot time, module entry time, elaboration time,
	function call time, block entry time, expression evaluation time and statement 
	execution time. virtual address translation happens here, one instruction at a time.

static bindings occur before run-time, 
dynamic bindings occur during run-time.

early binding generally implies greater efficiency
later binding generally implies greater flexibility

Object Lifetime and Storage Management:

Creation and Destruction of Objects
Creation and Desturction of Bindings
Deactiviation and Reactivation of bindings
Refrences to variables, functions, and types.







