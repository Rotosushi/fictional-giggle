
prefix: or setting up some language

	if we keep with the lines drawn in the language
	by CDL, the language is composed of two things,
	functions and atoms (which is
	simply an object considered together with its type;
	CDL explicitly refers to is as 'an object with it's type'
	and then bemusingly complain that they have no grouping noun,
	well, atoms it is!
	and functions they call algorithms, but it's 2019, and that language
	is not how we talk about programs now.)

	functions and atoms have three important attributes
		name
		value
		type

	the name is how we refer to any given entity in the text of
	our program. names are bound to their type and value within 
	the current environment. the storage location of the value
	will depend on both the kind of entity and where textually
	it is stated.

	the value is the encapsulated state of the entity
		-> this can be as simple as an integer, to something
		more complex like an array of pairs, or any given
		state that a given type would be associated with.

	the type is the encapsulated behavior of the entity
		-> this is the functions arguments mapped to its returns,
			T -> T
			or the set of operations valid on an instance of that type.
			the type is used to deduce which functions the programmer
			is referring to textually. each type is associated
			with some particular configuration of state. the type
			is used to deduce how it the state could be/is configured.

in other words; the type holds the set of valid internal representations (value)
	for an instance of that type, and the set of valid operations on that type.

	functions and atoms together we call entities.

	an entity can be either
	primary, or
	composed
	
	we call an entity 'primary' if, given the syntax, semantics, 
	and current level of abstraction, the entity cannot be sensibly
	decomposed into other entities. in example 'int', the addition
	of two integers requires descending a level of abstraction and
	considering the assembly instructions required, this is clearly
	something that a programmer in a systems language does not
	want to concern themselves with, hence all programming languages
	(above assembly) have an abstraction which denotes the addition
	of two integers, and the programmer can concern themselves with
	the interoperation of language components.
	
	a composed entity is anything that could further be decomposed into
	composed or primary entities. i.e: an algebraic data type, an array,
	a stream object. this composition of parts to make a whole is
	a common theme of all languages.
	
an entity can further be categorized as
	concrete
	abstract
	
	a concrete entity is one that is provided by the language itself,
	it's primary data-types, it's primary functions (like c's for and while loops, etc...)
	
	an abstract entity is one that has been defined by the programmer

it follows from this that most primary entities are also concrete.
	
given these four ways of categorizing entities in the language, we can then
define common mechanisms in programming languages.	
	
1: the kernel
the kernel of a programming language is composed of it's
	primary concrete algorithms (=, +, etc..)
	primary concrete types (int, float, etc..)
	primary concrete objects (records, arrays, pointers, etc..)
	
2: construction mechanisms
programming languages usually provide construction mechanisms in order
to define and use entities in the language

	for functions there are control structures 
	such as 'if' and 'while', as well as 
	function call notation.

	for atoms there are data structures, as well as 
	the associated value constructors.
	
3: abstraction mechanisms
programming languages generally provide abstraction
mechanisms to build new primary entities 
	declarations for functions (functions, procedures, operators, etc..)
	declarations for objects (variables, constants, arrays, etc..)
	declarations for types (abstract data types)

4:
programming languages sometimes provide extension mechanisms, by which
a programmer can extend the semantics or syntax of the language by means 
of some entity(ies) in the language. (this is accomplished in lisp, and some lisp variants using macros) 
here is where i see the most opportunity for growth. if the parts of
the language can themselves be specified in a language, then we can
mechanize the extensions via the specification language. if the 
specified language can interface directly with the specification language.
and both languages share the same base syntax and semantics, only different
abstractions and types, then we can extend the specified language
by the specified language itself. this would require that the
specified languages kernel be open to extension.

all entities in a programming language can be categorized thusly

			|	primary (an entity that cannot be decomposed			|	composed (an entity formed of primary and
							at the current level of abstraction)		|				composed entities)
------------|-----------------------								----|------------------------
			|	functions:				| atoms:						|	function:				| atoms:
concrete	|	=, +, *, cmpxchg,		|	int, real, text,			|	  push, pop, peek,		|   stack, queue, list
 provided	|	typeof, sizeof, [],		|	predefined constants		|	  prepend, append,		|   map, set, file,
 by the		|	if, while, print		|	[], *, !*,					|	  open, seek, fork,		|   socket, thread,
 language	|							|								|
------------|-----------------------								----|-------------------------
			|	functions:				| atoms:						|	function:				| atoms:
abstract	|	macro, fcall,			|	local & global vars,		|	fn,						| type,
 written	|	 						|								| 
 by the		|							|	local and global consts		|
 programmer	|							|								|


so, with some definitions out of the way, typically a programming language
	is designed with a problem domain in mind. (and/or: abstraction layer)
	in order to better design pink's feature set I and going to examine c's.
	
	given new inspiration by finding : https://www.bell-labs.com/usr/dmr/www/chist.html

	c was designed to create computer systems,
	so it exists as a simple stack based language
	that interacts directly with the hardware. given its low level duties 
	it's kernel is correspondingly low level, its primary data types are
	(generally) the primary data types of the underlying assembly, and it's
	relatively simple and small definition make it not too much of an undertaking
	to port to a new hardware, given the fact that the collection of features
	that c brings to the table is generally enough to write a complete OS.
	(I say this thinking about Unix and currently Linux, where they are written
	in c that is relatively unchanged since it's inception. very recently (2018)
	c has gotten some much needed quality of life improvements.
	over time c has become a 'lingua franca' of
	computer languages, where many programming languages which operate
	at higher levels of abstraction away from the hardware than c; are using
	c to communicate with one another.
	all of these facts made c a very attractive	language whilst computers 
	were still entering the collective conciousness.
	c retains a lot of is current popularity as a result of its initial success,
	its solidly designed kernel, and the fact that the work of porting it
	to every hardware platform in existance has already been done.
	c also has design mistakes that make creating and managing large
	projects more difficult than they need to be. it also has portions of the design which interact
	in hard to conceptualize ways that have created some notoriously hard
	to debug programs.
	

	one of which is the existance of
	implicit type casting. simply on the basis that the integrity of the data
	is not assured in some special cases. if the integrity of the data is not
	ensured, then the type cast needs to be explicit.
	I think that if anything is going to be implicit,
	it shouldn't be the possible destruction of information.
	Type promotion is fine because the data is the same. however
	if you are passing a float into a function that takes an int that is
	one of the problems the type system was built to solve. with the case
	of casting a float to an int for some arithmetic operation, I would
	much rather the user leverage the overloaded add function to add two floats,
	and be explicit about their cast.
	 
	The void* is my next issue,
	the problems that it solves still need to be solved however. Issues with
	the void* arise from passing the wrong type of data into a function
	and then when the program attempts to access it as the wrong type
	a runtime exception is generated, or worse, the program fails silently.
	when I have used a void* in C,
	it has always been to write more generic functions. I feel that
	when I write a function taking a void*
	the function would always expect that void* to be one of a set of
	types, for which said function was relevant. I feel that the type system
	of the programming language can be leveraged to achieve something akin
	to this but in a type safe way. I have an idea to allow for some 
	sort of constraint to be applied to the parameters of a function. say, this test function must
	pass when applied to the type you tried to pass in order for the
	call to be considered semantically correct; and the test function would be able to be
	constructed of statements like, the type passed must be able to be added to or
	have some other operation defined, probably a user function, or a reflective entity.)
	another usecase is dynamic memory allocation, but I think reflection
	or macros would make this trivially easy to replace. something like:

	definition
	allocate ($typename) -> typename-ptr
	
	usage
	ptr-type <- allocate(typename)


	the good parts:
		w.1: most of the kernel of the language
		w.2: the static type system
		w.3: the stack based scoping rules
		w.4: expressions

	the bad parts:
		m.1: the textual preprocessor
		m.2: the header-file/source-file split
		m.3: null pointers (without default initialization even!)

	the parts which need work:
		q.1: pointers
		q.2: dynamic memory system
		q.3: composite types
		q.4: enums and switches
		q.5: type casting
		q.6: void*


	w.1: 
		most of the kernel of the language is good, the primitive types
		map naturally to a large amount of hardware in use today.
		if statements and the if-else if-else chain is convient 
		to express grammatically and a good control structure for 
		implementing functions. while loops are also a good abstraction, 
		giving access to both bounded and unbounded iteration.
		the switch statement is useful, but if it isn't using
		a specific assembly instruction it's just syntax sugar.
		the for loop is syntactic sugar, i don't mean this in a disparaging
		way, what i am more trying to say, is that something like 
		a for loop could be constructed out of a while loop and a macro.

	w.2: the static type system is good, and I think is 100% neccessary
		 for a systems level programming language. I think the syntax that
		 the programmer uses to express the type of something is a bit
		 clumsy in hindsight.

	w.3: the stack based language part of c is I feel a good abstraction.
		 it gives functions a natural expression, and requires very little
		 runtime overhead to implement (the c runtime is famously small).
		 making it ideal for small systems, 
		 this is an advantage over a managed language in the systems language
		 layer of abstraction. how much extesion the runtime requires to support
		 all of the features i would like to add is unknown to me. at some point
		 i will be comparing the two kernels.

	w.4: 
		expressions, or infix functions, are a syntacticaly light,
		grammatically unambiguous way of composing functions. I really
		like this syntax for function composition, even though it
		will be restricted to two arg w/a return argument functions,
		and one transient arg functions. (input type == output type)

	m.1:
		the textual preprocessor was a clever solution to a few problems,
	namespaces, conditional compilation , and macros. 
	The silent failure of the textual macro system can make debugging a nightmare,
	and the lack of type checking reintroduces all of the subtle type bugs
	back into the language.

	The design goals are in the right place however,

	programmers need some way of enforcing which
	symbols get exported from and imported to any given module, both to 
	facillitate code reuse and to give a finer grained control over the namespace
	of the current module. this can enable cross platform development for instance,
	with the choice of which implementation makes its way into the final exe, being
	made by the preprocessor. maybe textual manipulation is not entirely without merit.
	as this choice still needs to be made to support cross platform development.

	programmers also need control over which code gets compiled given some
	external constraints as well, this can be due to hardware differences
	or to versioning, or some other constraint, but is usefull too.

	the last use-case that I know about for the textual preprocessor
	is macros. the problem with macros mainly stems from the
	fact that the textual macro is implemented as textual substitution,
	hence is untyped, and cannot be type checked. this combined with some of the
	implicit-cast rules has created some very hard to catch bugs.

	i feel like there needs to be another feature of the compiler, whereby we can
	give more specilized hints to the compiler so that it can include or exculde
	different versions of the same function, or know that this variable refrences
	memory that could change externally, or maybe that this function is intented to
	be an interrupt handler. there needs to be communication to the compiler
	about information that cannot otherwise be inferred or known by semantic
	analysis, this is a systems language afterall, in essence this means
	that we assume the programmer knows more about the execution environment
	than the compiler does, or can know.

	m.2: 
		the header-file/source file split is the other half to c's solution
	to namespaces, namely the syntactic element of the language which
	specifies the public namespace of a given file in c. it has to be specified in 
	a separate file denoted the header file.
	this means that at a bare minimum an exported functions type definition
	needs to be typed out twice, it needs to be changed in two locations
	if you want to change it at all, and two files need to be updated
	which is twice the operation length, for which other tools need to be
	created to ease the use like intellisense.
	the repition of header files increases the viscosity of the programming language, 
	making matinence and upgrading harder and more tedious.

	m.3:
		null pointers are the source of many headaches and should be replaced.

	q.1:
		pointers however are a good abstraction, giving the language a way to
	express dynamic entities, as well as a way of interfacing
	directly with the hardware running the language. the syntax and
	semantics surrounding pointers needs updating however. null pointers
	are the source of some of the hardest bugs to catch, and are probably
	responsible for most runtime crashes in the language. I think that
	combining pointers with optional types gives pink an easy abstraction
	to use instead of nullptrs, either the pointer exists or the entire type is Nil.
	there can be no null refrence exception, because the language does not 
	create null refrences. this is different than the programmer creating
	their own invalid pointer by freeing the memory and then maintaining
	a pointer to the freed memory and accessing it, and in this case we could
	be pointing at free memory (which the runtime can catch and then throw
	an exception, luckily) or in the case of the ABA problem, that freed
	pointer is accessed after the memory it pointed to was reused by the
	program and now it points to 'in-use' memory, but it is not the data
	that we expect to find at that location. this can be especially bad
	if the program then writes to the ABA problem pointer. potentially
	corrupting multiple parts of the program simultaneously. what if the
	free operation flips the type of the pointer to Maybe Nil instead of
	Maybe Ptr. so typewise no operations are allowed on that name after
	it's memory is freed?

	q.2:
		the dynamic memory system of c is mostly fine, but it's interface in the language
	is very barebones. this is intended, as the language was designed to be
	a single abstraction layer above the hardware. however this barebones implementation
	leaves the task of maintaining pointer validity up to the programmer.
	this is a problem domain which can be solved at the language level, and should be.
	there are two important abstractions that this system relies on for
	successfull interoperation with the rest of the language; the void*
	and the heap. dynamic memory is a very difficult abstraction
	for new programmers. it's correct use relies on being intimately
	familiar with the control flow of your program, and of the current state
	of your program. it becomes especially difficult when you take into
	consideration the interoperation between components that know nothing
	of eachother, acting asychronously, and working upon the same memory.
	there soon are combinatorially more elements to hold in ones head, 
	which increases the likelyhood of errors and the difficulty of solving the
	errors you have. only having the choice of a heap as the default allocation
	strategy is not always great, mostly it's unnoticable to higher level abstractions.
	I'm not saying there shouldn't be a default allocation strategy,
	and im not even saying that the default	shouldn't be a heap,
	I am saying that we should do more testing and performance analysis
	to support the choice of a default.
	but i will say that there needs to be an option to change it, and i think the
	runtime either should not have as high-level an abstraction as malloc, or the
	underlying abstraction should be able to be specified via some semantics, or both.
