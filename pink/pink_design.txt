
%% Zen

pink is a language whose job is to allow programmers to easily
	manipulate assembly instructions as logical units and to 
	easily manipulate data around the machine efficiently.

I think the best way to accomplish this is with an imperitive language,
that makes a style based around functions the easiest to express.
(TL;DR prefer f(x) to x.f())

this means a few things:
	- algebraic data types
	- statically typed
	- pass-by-value semantics
	- make mutation the exception not the rule

- a small yet expressive language, built for readability, ergonomics
- and easy parsing.

- logical consistency, one obvious way of doing things,
- easy to say what is easy to do.

- The goal is also not to create a 'pure' language, nor is the language
	'impure', this language is a collection of the most sensible features
	working together to express programming ideas ergonomically. The goal 
	is to have a clear (read: unambiguous), 
	concise(read: just enough characters to communicate, but not so many
	 that it becomes a chore to write and mentally parse. see: c++ templates as an example of 'chore to mentally parse',
	 or dense functional code),
	 and highly expressive language; 

- Dimensions of the language
	- Expressivity
		the language should enable programmers solve their problems, rather
		than solve the riddle of how to express what they want to say in the
		language. i.e. "stop telling me what you are going to say, and just say it."

		Programmers fluent in the language should be able to easily read what
		one another are saying and doing in their code. 
		common abstractions should have a common form
		common tasks should have a common form
		it should be easy to say what is easy to do

	- Security
		The language doesn't exist in a vacuum, a robust, expressive language
		should be aware of the security concerns of a modern language. C is 
		exploitable because it is hard to write hardened code,  More complex languages
		are exploitable because their own operation is hard to understand, and
		security is hard because it is easy to exploit interoperations
		between disperate programs. 

	- Performance
		writing code that utilizes its resources effectively is difficult. writing
		interoperating abstractions that can utilize shared resources is difficult.
		performance modeling should be easy to do, and require minimal cognitive
		overhead. the shape of performance modeling is uniform in the time axis, 
		and on the space axis, so the language should have niceties.

	- Maintainability
		Code that is written today, may be talking to code that is written 20
		years from now. This is not something that can be ignored. How does this
		affect the other dimensions, and how does the language assist programmers
		in this dimension.

	- Testing
		testing is a non-trivial aspect of development, that also has a uniform
		shape. How does this language assist programmers in testing their own code?
		in testing unknown code? 


besides the language concerns, there are concerns general to any abstraction.
these are intended to be used as the dimensions above are used, as guiding principles of
the design. any given language entity must make a case for it's existance, the strength
of which should be proportional to the distance to the core of the language, and how important
the proposed entity is to be. what adds to an entities case for existance
is a strong showing in the core principles of the language, i.e. it helps write performant code,
or secure code, or helps maintain code, or some such. what can then add to the strength of the entities case
is a strong showing in the dimensions of abstraction itself. i.e. the feature reduces the repitition
viscosity of code maintinence. this then gives adopters of new entities a new way of comparing
proposed entities, and to seek out entities which have certain properties.

to speak in terms of an abstract instance, should a language feature be proposed that then
subsequently everyone will use, then it better have a strong case for existance, not only 
from the perspective of how reliable the core tools need to be, but also from the perspective
of getting users to adopt a new way of doing things. should a new entity be used by one
person in one project, then that entity doesn't need as strong a case for existance. it only
needs to convince that one person, and only needs to be useful in the one project.

- Cognitive Dimensions of Abstraction

Abstraction gradient 
    What are the minimum and maximum levels of abstraction exposed by the notation? 
	Can details be encapsulated?

Closeness of mapping 
    How closely does the notation correspond to the problem world?

Consistency 
    After part of the notation has been learned, how much of the rest can be 
	successfully guessed?

Diffuseness / terseness 
    How many symbols or how much space does the notation require to produce a 
	certain result or express a meaning?

Error-proneness 
    To what extent does the notation influence the likelihood of the user making 
	a mistake?

Hard mental operations 
    How much hard mental processing lies at the notational level, rather than at
	the semantic level? Are there places where the user needs to resort to fingers 
	or penciled annotation to keep track of what’s happening?

Hidden dependencies 
    Are dependencies between entities in the notation visible or hidden? Is every
	dependency indicated in both directions? Does a change in one area of the 
	notation lead to unexpected consequences?

Juxtaposability 
    Can different parts of the notation be compared side-by-side at the same time?

Premature commitment 
    Are there strong constraints on the order in which the user must complete 
	the tasks to use the system?

    Are there decisions that must be made before all the necessary information
	is available? Can those decisions be reversed or corrected later?

Progressive evaluation 
    How easy is it to evaluate and obtain feedback on an incomplete solution?

Role-expressiveness 
    How obvious is the role of each component of the notation in the solution as a 
	whole?

Secondary notation and escape from formalism 
    Can the notation carry extra information by means not related to syntax, such 
	as layout, color, or other cues?

Viscosity
    Are there any inherent barriers to change in the notation? How much effort 
	is required to make a change to a program expressed in the notation?

    This dimension can be further classified into the following types:

        'Knock-on viscosity' : a change in the code violates internal constraints
			in the program, whose resolution may violate further internal constraints.

        'Repetition viscosity' : a single action within the user’s conceptual 
			model requires many, repetitive device actions.

        'Scope viscosity' : a change in the size of the input data set requires 
			changes to the program structure itself.

Visibility 
    How readily can required parts of the notation be identified, 
	accessed and made visible?

creative ambiguity 
	does the notation encourage interpreting several meanings of 
	the same element?

indexing 
	are there elements to guide finding a specific part?

synopsis 
	"gestalt view" of the whole annotated structure

unevenness 
	some creation paths are easier than others, 
	which bias the expressed ideas in a developed artifact.
	


TLDR:
- "Stop telling me what you are saying and just say it!"

- "If you liked the code you should have put a test around it."

- "Don't bring up performance as a talking point unless you have the
	data to back up the claim."

%% ~Zen

%% top level thoughts:

A program in pink is composed of declarations and statements
	Declarations:
		A declaration adds a new item to the current scope.
		This applies to
			function declarations
			variables
			unstructured scopes ( aka the unnamed scope { // } )

	Statements:
		A statement is something that has a side effect, either upon the state of a variable
			or the state of the program flow
		This applies to
			function calls
			operators (=, +, -, etc.)
			structured scopes (conditionals & loops)

Why make a new programming language?
	for fun?
	for profit?
	how about:
		There is a ton of unneccessary complexity in modern day languages
		c is in its verbosity (This is a result of being a small lang,
			which is addmittedly also a plus), security/memory pitfalls, and age.
		c++ fails in its verbosity (partly inherited, mostly added),
			semantic and syntactic overhead, and its plethora of features (there's just too many!
			i don't know what the right amount is, but c++ has too many.
			if there is four ways of doing/saying the same thing, only one of
			those should be primitive to the language.)
		Dynamicly Typed and Garbage Collected languages fail in that
			the computational overhead of garbage collection is unnaceptable
			in low level code and embedded code, especially so in RTOS situations.
			so a systems language should not force one upon you. (however,
			in some cases you may want one, so maybe some grammars can be
			garbage collected, maybe you can write in a garbage collected dialect.)
			and they fail with the mental/debugging overhead of duck typing.
			this introduces subtle errors back into the language that static
			typing does not allow.

If these are the problems, then how is this language a solution?
	what are the design goals of pink?
		1) The language should have a small, focused feature set which can be learned
			quickly, and mastered slowly. Each feature should hold its own weight in the
			design space of the language. There shouldn't be 12 innate ways of doing one thing,
			the programming language should have a convienient built-in, with the ability for the 
			programmer to specialize should they have a unique set of constraints.

		2) The syntax of the language should make it easy to say what is easy to do.
			Taking a page out of the python school of thought, the language should be designed for
			_humans_ and not the computer. The syntax should help the programmer to understand what is happening
			not obfuscate the ideas (looking at you c++ templates)

		3) The syntax and the semantics of the language should help the programmer build
			the necessary abstractions for small and large programs in a way that is 
			computationally effecient for the machine, and cognitively efficient for the programmer.

		4) The language should help you with the hardest of problems! 
		~Multithreading & Parrellelism~
			
		~Dynamic Memory management~
			
		~Hardware interface~

		~Cross Compilation~
			
		~Interlanguage Interface~ 

		~Debugging~

		~Testing~

		~Maintinence~

		


%% Thoughts on things
-- logical consistency : a function declaration should always look
	like a function declaration, a struct definition should always
	look like a struct declaration, a variable should have the 
	constraints that you tell it to have, this should permeate the
	design of the language. (instead of '~' and '!' we have '!' and '!!',
	because this lines up with the operator consistency of '&' and '&&',
	'|' and '||', and '^' and '^^' ((also yes, this does change '!=' to
	'!!=')))

-- logical consistency vs. symbol overloading
	TODO: write this

-- Type Constructors
	<type-name> '(' <argument-list> ')'

	type construction seems like a necessary language feature.
	c doesn't have this, and so requires seperate intitialization
	functions for more complex types. written and maintained fully by the programmer
	every type is constructed in some way, and humans are lazy, so this is an area
	where a lot of unnessecary bugs occur. bugs because the programmers function
	becomes out of date if the underlying representation changes, which can permeate
	throughout the code. and unnessesary because the compiler understands it's types
	enough to generate assignment semantics for it's underlying types, which means that
	it can generate the code for composite assignment semantics. 
	(shallow copy non-owning refrences, deep-copy owning refrences)

	var : my_struct;
	init_my_struct(var, ...);

	-vs-

	var := my_struct(...);
	

	This separation of the type
	from it's intializer creates cognitive overhead. If types
	always have a constructor it reduces the cognitive overhead
	of ensuring that types are valid after declaring them.
	dynamic memory allocation should not occur by default. 
	The default constructor should allocate
	space on the stack frame of the current function.the semantic rules around static variables should remain 
	largely consistent with c. 
	

-- initializer lists for structs
-- declared constructors like jai 

-- struct a :: {b: int, c: string}

	v1 : a{someint, somestring};
	v2 : a(someint, somestring);
	v3 := alloc (a, someint, somestring);
	v4 : a;
	v4 = alloc (a, someint, somestring);

	new/delete vs alloc/dealloc vs ??

	var := #alloc <type-name> <argument-list>;
	
-- typecast
	(<type-name>)<identifier>
		i argue bad; because it isn't explicit that there is a cast

	option 1:
		'cast' '('<typename>',' <identifier>')'
		- reads like a function call
		- could be implemented using planned feature: reflection
			()
-- a function call is so far the only syntactic construct that looks 
	like : <identifier> '(' <argument-list> ')' ';'
	if type construction calls a function, then the semantic difference
	between <function-name> '(' <argument-list> ')' ';'
	and     <type-name>     '(' <argument-list> ')' ';'
	might not be a problem.
		aside: This also seems like part of an argument for overloading
				functions as a basic feature.
	a function is always 'stored' in a variable, but we don't always
	want programmers to change top level functions, this is solved in
	two ways, first the 'fn' declarator. so you can say

	fn f (arg: int) -> {return arg * arg;}

	and second, constant binding to a variable

	var :: (arg: int) -> {return arg * arg;};

	note: declarations always have an ending ';'
			whereas the function definition, much like
			the record, do not have an
			ending ';'. 

-- defining and calling functions

	fn func :: (a: int, b: int) -> (int) {return a + b;}

	fn apply :: (a: [] int, b: int, f: (int, int) -> (int)) {
		for a { 
			it = f(it, b);
		}
	}

	fn apply :: (a: [] float, b: [] float, f: (float, float) -> (float)) {
		for a {
			it = f(it, b[it.index]);
		}
	}

	

-- constness is always indicated through the '::' operator.
	constant functions:
		fun :: (a: int, b: int) -> {return a + b;}

	non-const functions:
		fun := (a: int, b: [] int) -> {return for i in b {c += a + i}}

	constant type definitions
		record my_type :: { v: int; w: float; }

	constants in code
		my_constant :: "this string is constant";
		mY_other_constant :: 42;

-- pointers
a major departure from c pointers is that in pink,
all pointers are option types.
meaning instead of signaling a deleted refrence with
a null pointer value, the type of the pointer will be
null instead.

	two kinds, both nilable, 
	non-owning pointers (refrences) *
	owning pointers     (pointers) !*

	?? refrence counted pointer (counted refrence) #*
	   :gives the same behavior as c++ shared_ptr ?? 

	refrences are just that, a refrence to some memory, 
		
	pointers own the memory they point to, when they go out of
	scope, so does the memory.
		:these have all the power of a pointer, but negate a lot
		of the pitfalls of c-style pointers, much like
		unique_ptr in c++.


	// variable capture v. dynamic scope
	/// does variable capture help? or does it conflict
	    with internalized scoping rules too much?
	
	h :: () ->
	{
		variable := "smelly";
		{
			variable := "hello"; // declares a new variable
			print(variable); // prints "hello"
		}
		print(variable);     // prints "smelly"

		{
			variable = "goodbye"; // error: variable is not declared in current scope
		}

		[variable]{ // here we capture the refrence to the outer variable
			variable := "code"; // reassign the contents of variable
			print(variable);    // prints "code"
		}
		print(variable); // prints "code"
	}

	vs:

	h :: () ->
	{
		variable := "smelly";
		{
			variable := "hello"; // error: variable already defined
			print(variable); // prints "smelly"
		}
		print(variable);     // prints "smelly"

		{
			variable = "goodbye"; // reassigns variable to goodbye
		}

		[variable]{ // here we capture the refrence to the outer variable
			variable := "code"; // reassign the contents of variable
			print(variable);    // prints "code"
		}
		print(variable); // prints "code"
	}

	// what does polymorphism look like?
	// double := (x: $T) -> {return T * T;}
	// x := 42;
	// y := double (x);

	// do we want function overloading?
	// double := (x: int) -> {return x * x;}
	// double := (x: float) -> {return x * x;}

	// templates allow you to abstract the types
	// within a function.
						// syntax from Jai
	mytemplatefunc :: (x: $T, y: T) -> {return x * y;}

	the constraint on the type is that 'x', 'y' are the same type, derived from 'T';
	implicitly, the derivation is from the type of 'x', the type of x must contain 
	a function definition for '*'

	x := 3; y := 4;
	z := mytemplatefunc (x, y);

	x := 3.14; y := 5;
	z := mytemplatefunc (x, y); // is this then a syntax error?
								// the types are different, even though
								// through an implicit type conversion
								// this function call can work, and without
								// a loss of precision.

	x := 4; y := "not-an-int"
	z := mytemplatefunc (x, y); // This has to be a semantic error,
								// {int} '*' {string} 
								// is not a valid operation

	// function overloading allows you to abstract
	// the body of a function

	overloadfunc :: (x: int, y: int) -> {return x * y;}
	overloadfunc :: (x: float, y: float) -> {return x * y;}

	macros:
		a macro is not code itself, but a specification of the
		form of code. the idea being that code itself can be something
		that code can talk about. normally we talk about numbers, strings
		and other data-abstractions within code. we can also talk about control
		flow in the form of functions and conditional statements.

		c/c++ have macros, they are all handled with the preprocessor.
		 This means that they operate only with textual substitution.
		 this is okay for simple forms of macros, but lacks typechecking
		 and much of the power of lisp macros.

		 c++ has recently developed something with close to the same power 
		 as a true macro, and it is in the form of templates. in their original
		 conception templates are a way to abstract the type of a procedure.
		 which implements a form of parametric polymorphism.
		 this is because many algorithms work regardless of the
		 type that they consume. data-structures in particular Linked lists, Arrays, Heaps, etc.
		 in other words, there is a -pattern- to the code, and the storage 
		 structure has the same form no matter the contents. The form of the 
		 storage medium is the pattern which is captured within the template.

		 now, with more complex structures the constraints that are placed on
		 what the function can operate on will often differ from the basics of
		 a type. (it's size, and name). sometimes a pattern will only work
		 if the underlying type supports certain semantic operations upon it.
		 say, a macro which only makes sense if the type we are operating on
		 can be iterated over. the compiler will not know a-priori which
		 exact underlying semantics that a user will utilize in their macro.
		 so the macro facility will need some way of specifying that.
		 the current solution is to have the argument of the macro itself
		 associated with some function which reflects over it's passed in
		 type and enforces some constraints upon the type passed in.
		 should the constraints pass, then the macro can be safely expanded.
		 should the constraints fail, then the compiler can emit a semantic error.
		 the user can specify the constraints that each argument of the macro
		 places on it's agument. if the user fails to supply any constraints
		 that should still be fine, macros will be typechecked after they are
		 expanded once given the arguments. meaning if the underlying type doesn't support
		 the semantics which the macro expanded with, then the compiler again gets the opportunity
		 to emit a semantic error. this provides two places where a macro can be typechecked.
		 and gives users a powerful tool for ensuring that their macros will be used correctly.


	// Helper functions
	(defun primep (number)
	  (when (> number 1)
	    (loop for fac from 2 to (isqrt number) never (zerop (mod number fac)))))

	(defun next-prime (number)
	  (loop for n from number when (primep n) return n))

	// what we want the call to look like
	(do-primes (p 0 19)
	  (format t "~d " p))

	// semantically equivalent hand-rolled version
	(do ((p (next-prime 0) (next-prime (1+ p))))
	    ((> p 19))
	  (format t "~d " p))

	// an example macro definition
	(defmacro do-primes (var-and-range &rest body)
	  (let ((var (first var-and-range))
	        (start (second var-and-range))
	        (end (third var-and-range)))
	    `(do ((,var (next-prime ,start) (next-prime (1+ ,var))))
	         ((> ,var ,end))
	       ,@body)))

	// equivalent macro form, with more explicit parameters
	(defmacro do-primes ((var start end) &body body)
	  `(do ((,var (next-prime ,start) (next-prime (1+ ,var))))
	       ((> ,var ,end))
	     ,@body))



	// ad-hoc polymorphism
		adhoc :: (x: int) -> {return x * x;}
		adhoc :: (x: float) -> {return x / x;}

	// template polymorphism
		temp :: (x: $T) -> {return x * x;}

	// macro macr :: (x: $T, y: op) -> {return x y x;}

	// macro macr :: (x: {type}, {binop}) -> {return x {binop} x;}

	// macro macr :: (x: {type}, y: {lambda}, z: {static-int}) -> (list: []{type}) {
	//	l: []{type};
	//	for(i := 0; i < value; i += 1) l.prepend((y, x));
	//	return copy(l);
	//}

	// 'for' '('{declarations} ';' {conditional} ';' {post-statements}')' {statements}
	//		-> { 
	//			{declarations} 
	//			while ({conditional}) {
	//				{statements}
	//				{post-statements}
	//			}

	// 'while' '(' {expression} ')' {statements}
	//		-> {
	//				w: if ({expression}) {
	//					{statements}
	//					goto w;
	//				}
	//			}
	
	// 'if' '(' {expression} ')' {statements} 
	//		-> {
	//				jump-if-not-equal i, {expression}, {true-value},
	//				{statements}
	//				i:
	//			}




I haven't coded much assembly (read, a few small programs), 
but the structure of large assembly
programs probably looks fractiline to the structure of
the compiled source code being emmitted by any major language today.
(if it's an interpreted language, then try: the side effects of both
 programms are similar.)
 assembly still has primitive types, functions with parameters,
 conditionals, and loops. composite data-types like records and
 arrays are still used. so then what is the purpose of the higher
 level language? In my mind: to abstract away unnecessary details of
 the hardware in the hope that we improve the quality and speed
 at which we can develop. a programmer doesn't need to consider
 how to create, store, and modify local variables, they only need
 to know how to interact with these variables. you don't need to
 know how to create a function call stack on the current hardware,
 you just need to think about how the functions interact. This is
 a good abstraction layer to have, in my mind and i assume, in the 
 minds of anyone who has had to code in assembly.

 what is the purpose of abstraction? In my mind the same as what is 
 achieved by the programming language. abstraction should create a new
 tool, or set of tools which can ergonomically express the ideas that
 are vital to the operation of some thing. technically the CPU is doing 
 one thing at a time, one step at a time; and in order to ergonomically
 use the CPU we need ways of specifying what the CPU should be doing
 and we would like those abstractions to be as easy to use as possible.
 we also want to be able to say anything we want, as long as it makes sense.

///
/*
guiding principles of the order of evaluation:
	one wants fully evaluated expressions before assignment

	one wants to test the value of fully evaluated
		expressions.

	one wants to bitwise operate using the value of fully evaluated
		numerical values

	one wants pemdas to hold for numeric operations

	one wants prefix expressions to bind to the immediate
		next grapheme

	one wants postfix expressions to fully evaluate before any
		operations happen on those values

	A = B, C = D, E = F
	-> (, (A = B) (, (C = D) (E = F)))
	',' binds looser than '='; so that EQ expressions
	 are kept together.

	A | B ^ C & D	
	-> (A | (B ^ (C & D)))

	A && B || C ^^ D	
	-> (A && B) || (C ^^ D)

	A + B == C - D	
	-> (A + B) == (C - D)

	A + B && C & D	
	-> (A + B) && (C & D)

	A + B | D
	-> ((A + B) | D)
	
	A - B ^ C | D
	-> ((A - B) ^ C) | D)
	
	!A ^ B & C()
	-> (!A) ^ (B & (C()))
	
	!A ^ B | C()
	-> ((!A) ^ B) | (C())

	A & B < C | D
	-> (A & B) < (C | D)
	
	A & B < C ^ D | E	
	-> (A & B) < ((C ^ D) | E)

	A < B == C < D
	-> (A < B) == (C < D)

	A + B | C == D	
	-> ((A + B) | C) == D 
*/
/*
Token := (<op1> | <op2> | .. | <opn> )
	-->
		if ( <<lookahead-predicts-op1>> ) { <<match-op1>> }
		else if ( <<lookahead-predicts-op2>> ) { <<match-op2>> }
		..
		else if ( <<lookahead-predicts-opn>> ) { <<match-opn>> }
		else <<unknown-token-error>> // no viable alternatives

	Token := (optional-opt)? (op1 | op2)
	-->
		if (<lookahead-predicts-optional-opt) { match-optional-opt }
		if (<lookahead-predicts-op1) {match-op1}
		else if (<lookahead-predicts-op2) {match-op2}
		else <unknown-token>

	Token := (one-or-more-opt)+ (op1 | op2)
	-->
		do { // ( .. )+
			<<code-matching-one-or-more-opts>>
		} while ( <<lookahead-predicts-an-alternative-of-the-one-or-more-opts>> )
		if (<lookahead-predicts-op1) {match-op1}
		else if (<lookahead-predicts-op2) {match-op2}
		else <unknown-token>

	Token := (zero-or-more-opts)* (opt1 | opt2)
	-->
		while (<<lookahead-predicts-an-alternative-of-the-zero-or-more-opts>> ) {
			<<code-matching-zero-or-more-opts>>
		}
		if (<lookahead-predicts-op1) {match-op1}
		else if (<lookahead-predicts-op2) {match-op2}
		else <unknown-token>
*/

%% ~Thoughts on things

%% Goals
// v1 goals : the static language
- functions with primitive types
- conditionals and loops with primitive types
- variables with primitive types
- expressions with primitive types

// v2 goals : the dynamic language
- pointers
- composite types
- dynamic memory

// v3 goals : features!
- polymorphism
- macros (programmer ast manipulation) 
- reflection
- concurrency

// vn goals : optimization
- make the compiler fast
- make the output code fast
%%

// research
- processor primitives
-- compare-and-swap, atomic load/store, atomic rmw

- dynamic vs. static vs. capture scope
-- maybe allow the programmer to specify?

