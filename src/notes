

Just enough to say what is interesting,
  no more.



  one of the problems that is starting to become clear
  is the tension between what the typechecker calls
  a valid program, and the actual semantics which
  arise from the body of functions. the raison d'etre
  of typechecking is to ensure that a typeable program
  is a program that will not generate runtime errors.
  {source: https://mitpress.mit.edu/books/types-and-programming-languages }
  however, this is not possible through tree analysis
  of the program alone, the sources of error are too
  varied and unknowable for static analysis to cope with.
  however, this does not disqualify the type system from
  existance, it simply points out that the class of
  errors which are knowable through static analysis is
  some subset of the full set of possible errors.
  the really nice thing about static analysis however is
  the properties that it shares with mathematics in general,
  namely, it is deterministic and replicable. (depending of course
  on the specific typesystem, this is words about some idea of a
  typesystem, not specific typesystems, or about judgements which
  are undecidable.)
  in layman's terms, it always finds the errors it is looking for
  if they exist in the source text; and since the typesystem runs each
  time we attempt to compile or execute the program, so it is always
  making judgements about what code exists now. it's judgements cannot
  become outdated in the sense that a programs comments can become
  outdated. for the programmer to change the typesystem's judgements
  they must necessarily change the source text itself.

  these two ideas of the program's semantics can become
  unaligned in specific cases.
  For instance consider the UNIX system call: exit,
  which in the language of Pink
  has the type Int -> Nil, that is it's a function
  that looks like it takes an Int value and returns
  nothing. however the actual result of calling this
  function is that the caller is taken off the
  scheduler's list and stops executing.
  now, this may seem like a minor semantic difference,
  especially since you as a programmer have to type
  exit(some-num); and are presumably writing a path
  of execution that intentionally calls exit.
  however, to the compiler exit is just another function,
  that can be called, so if a programmer were to make the
  error of calling exit, and then writing code after that
  call expecting the program to evaluate that semantics,
  they would be dissappointed to find out that code
  existing after the call to exit will never be able to
  execute, it is in effect unreachable, and dead. now, in the
  knowable error case of the programmer writing code
  after the execution of a return statement, the compiler
  can issue a warning, it can delete or never compile
  any code that appears dead in the source text in this
  manner, and in general the compiler writer can choose
  to preform actions when the case arises. however in the
  case of an operating system abstraction such as the call
  to exit the compiler cannot know that a call to that
  system function will result in the program stopping
  execution, and therefore it cannot issue a warning,
  nor can it remove the dead code from the program source.
  in effect the error cases that can arise from using
  the operating systems abstractions is by definition
  larger than the set of errors that can arise from
  using the language abstractions, just when considering
  the two from a theoretical perspective.
  however the larger issue in the design of the language,
  and really in the design of any language is that
  the semantics of the program source text is only ever
  able to be partially checked for correctness.
  the gap that exists between what the semantic analysis
  can perceive and manipulate and what the programmer can
  perceive and manipulate is large. and if we want to
  automate checking the correctness of the programs semantics.
  we are attempting to bridge that gap, this can be begum by
  writing tests. but it will soon become apparent that
  wether or not your question can be asked systematically has more to
  do with what question you are attempting to answer than
  what problems you are trying to solve.



how does operator overloading interact with
template/parametric-polymorphic functions
in the case of concrete vs abstract entities?

to begin in answering that question:
https://www.researchgate.net/publication/221252351_A_Calculus_for_Overloaded_Functions_with_Subtyping


question:
  what tasks in particular force the c language programmer to
  drop to assembly, and why can we not envision sensible
  abstractions which encapsulate these tasks in a higher level
  language?


one task is the context switch.
  where the operating system takes one task off of the running
  state and stores it in memory, then loads a new task into
  the running state. doing this requires directly talking about
  the registers in use by the current program, and the registers
  in use by the new program. (as well as a myriad of other OS
  subtleties I am sure.)

  why does c fail?
  the programmer has no way of
  directly referencing the data in any of the working
  set of registers.
  essentially we can't say "save register rax into memory"
  in the language of c.
  why does c not provide some sort of a built-in?
  well, the notion of a hardware register is
  not directly knowable at the level of abstraction
  in which a programming language operates.
  the very notion breaks from the idea of operating
  a level higher up from the hardware itself.
  we can make sense of the statement rax = some-word-sized-value;
  from a language implementation
  perspective, it is difficult though not
  impossible to envision what changes to the compiler
  is required to allow the programmer to store
  and directly access the active register set.

  specific compilers have specific ways of inlining
  assembly, but that means referencing the mete-material
  of your compiler, and this binds your project even tighter
  to which specific compiler you are using which could
  have consequences. (and you have already decided to write
  assembly, so you throw portability out of the window for that
  particular component.)

  you can always write a separate
  assembly source file and call your assembly like a c-function,
  (using your favorite assembler)
  though this requires knowing the c-abi of your
  particular operating system and it assumes that
  your language has a c foreign function interface,
  which admittedly plenty of languages do.
  (c's abi is very straightforward)
  additionally, assembling a separate assembly
  file complicates your build process somewhat.

  what really throws a wrench in the works
  however is our goal of portability. which
  specific registers you have access to is a matter
  of which exact specific CPU you have, and is usually
  only knowable by referencing the exact relevant documentation.

  so, these two factors (portability negation, having to
  descend a level of abstraction to get things done), to me
  point to the solution living outside of the kernel of the
  language. if we can construct the relevant semantics
  to speak assembly, or somehow give direct handles which
  can be accessed by the understood abstractions, but do it
  by composing together the more base abstractions that we
  have available in the language already. like, maybe
  we define what a base CPU type would need to provide
  through an abstract type a'la ML, and specify what the
  writer of a conforming CPU structure should export
  like, general purpose registers, floating point registers,
  and status registers, then a conforming implementation of
  a specific CPU structure could conform to what is specified,
  but also export more signatures which could represent the
  specialty features of the CPU or specialty hardware components.
  then, programmers could utilize that abstraction to write low
  level code, that interacts directly with the hardware, while
  saying it in the syntax of the language. and, if you want to
  write code that preforms a task switch you can do so against
  the abstraction, which hopefully can compile down to some minimal
  assembly.


another task is driver development
  when talking directly to hardware, most of what is
  being done is managing state. some but not all hardware
  is built based on a standard (or de-facto-standard) abstraction,
  which implies that there is some abstract data type which
  could encapsulate the state management and provide some
  nice interface for the language. in example, one could
  imagine a UART device interface that provides a memcpy
  style interface where the systems programmer just has to
  specify which array of bytes they want written to the device
  and a single call handles that for them. so, this immediately
  runs into complications when considering implementation of a memcpy style
  interface with a UART device. it would seem to imply that the programmers
  code stops running until the UART device was done transmitting.
  a trait that we probably do not want to have, given that UART
  data transmission is orders of magnitude slower than the CPU.
  in order to facilitate some sort of time disconnect
  there are a few different abstractions that one can choose from,
  you could describe it with coroutines, async/await,
  threads, or you could leverage the interrupt mechanisms of the
  CPU, each choice has a distinct effect on the shape of user code.
  and on the semantics of user code in some cases.
  in my mind, a good systems language would give the tools in such
  a way that programmers can write each effectively, and a clever
  api programmer could insulate user code from the difficult choices
  allowing learning programmers to utilize the hardware they have
  in a sensible, safe, and (hopefully) as fast as the hand-rolled
  assembly could hope to be.

  parts of this problem that add layers of complication:
    there are lots of different hardware protocols!
        which ones do we support?

    those de-facto-standards make for inconsistent hardware
      designs, where some hardware claiming to implement the
      same standard transmission protocol like SPI, could
      implement the standard in subtly different ways,
      such as different STOP bit patterns, different START
      bit patterns, parts of the standard are unimplemented, etc.

        how the heck could any single algorithm be written to account
        for all those subtleties?
        essentially, we can't have
        "the SPI algorithm"
        we have to have
        "the <device-specific> SPI algorithm"

        but could we imagine
        "the conforming SPI algorithm functor"?
        and then specializing it per-device?
        because if we can, that can help to insulate
        programmers code from driver code.











notes about parsing with bison:

the type here (%nterm) needs to be able to directly
represent each node in the grammar.
of which there are 5.

in the trivial calculator, we can get by with
each term having the same type. namely int.

here we need to express the abstract syntax tree
in terms of a c-style union. which is an untagged
union. since we are compiling in c11 mode I think,
we can specify unnammed unions in the ast, so we can
just say, AstTypeName.nil instead of refrencing it
through AstTypeName.union.nil
making our declarations here look more like we are using
the regular bison union, but each AstNode also tracks its
kind and type making the rest of the compilers job more
straightforward.
(spoiler alert, we can't)

to paraphrase the bison documentation

%nterm <typename> nonterminal

this statement declares the nonterminal to be represented
by the name 'typename' within the union type 'api.value.type'.
additionally any number of nonterminals can be declared here
and all will have the specified type

my question:
  what is the type bison selects for %token directives that
  are used trivially within the grammar? especially in the
  circumstance in which I have specified my own custom type
  in api.value.type

the best response I've found from the documentation:
{from: https://www.gnu.org/software/bison/manual/html_node/Mfcalc-Declarations.html}
  "Since values can now have various types, it is necessary to
   associate a type with each grammar symbol whose semantic value is used."
   "The Bison construct %nterm is used for declaring
   nonterminal symbols, just as %token is used for declaring
   token types. Previously we did not use %nterm before because
   nonterminal symbols are normally declared implicitly by the
   rules that define them. ~~~But exp must be declared explicitly
   so we can specify its value type~~~"

My assumed answer:
  don't worry about it unless you are relying on the terminal/nonterminal
  symbol having some particular type, in which case specify
  that type with a %nterm, %token, or %type directive.

  if you attempt to access the value of a non-terminal or token that
  does not have a type associated with it, bison will reject that
  grammar.

this raises an additional question,
  if 'typename' is the expected name of the union
  field associated with the terminal/nonterminal symbol,
  then what about places where we parse some sub-field
  of an entity?
  such as, i am reading the name of the argument, i would
  like that stored as a trivial char* until i can record it
  into the full lambda ast node, after consuming that input.

  the way the documentation is worded makes me assume that
  even if i add some new terminal symbol it will be represented
  by a full Ast node by bison? or maybe they secretly aren't
  but only nonterminal lhs values are constructed as Ast nodes.

  this is why the %token <typename> TOK
  directive exists.

the final verdict:
  all of these issues are solved by using pointers to a union value
  and only making bison differentiate between simple char*'s and
  Ast* nodes.
  when we extend the data we need to track to strings and integers
  and floats and such, we must extend the grammar and probably
  the underlying data-representation to account for the new data.
  we will distinguish between Ast* nodes, char*'s, Ints, and Floats,
  but again, -we- should manage that, and constrain the LALR parser
  to just construct the AST.
